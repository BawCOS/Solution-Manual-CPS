[["index.html", "Solutions for Computational Probability and Statistics Preface 0.1 Book Structure and How to Use It 0.2 Packages 0.3 File Creation Information", " Solutions for Computational Probability and Statistics Ken Horton Kris Pruitt Bradley Warner 2021-03-10 Preface Contained in this volume are the solutions to homework problems in the Computational Probability and Statistics book. 0.1 Book Structure and How to Use It This solution manual is setup to match the structure of the accompanying book. The learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for: Developing probabilistic models Developing statistical models for inference and description Advancing practical and theoretical analytic experience and skills 0.2 Packages These notes make use of the following packages in R knitr (Xie 2020), rmarkdown (Allaire et al. 2020), mosaic (Pruim, Kaplan, and Horton 2020), mosaicCalc (Kaplan, Pruim, and Horton 2020), tidyverse (Wickham 2019), ISLR (James et al. 2017), vcd (Meyer, Zeileis, and Hornik 2020), ggplot2 (Wickham et al. 2020), MASS (Ripley 2019), openintro (Ã‡etinkaya-Rundel et al. 2020), broom (Robinson, Hayes, and Couch 2020), infer (Bray et al. 2020), ISLR (James et al. 2017), kableExtra (Zhu 2020), DT (Xie, Cheng, and Tan 2020). This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 0.3 File Creation Information File creation date: 2021-03-10 Windows version: Windows 10 x64 (build 18362) R version 3.6.3 (2020-02-29) References "],["CS1.html", "Chapter 1 Case Study 1.1 Objectives 1.2 Homework", " Chapter 1 Case Study 1.1 Objectives Use R for basic analysis and visualization. Compile a report using knitr. 1.2 Homework Load tidyverse,mosaic, and knitr packages. library(tidyverse) library(mosaic) library(knitr) 1.2.1 Problem 1 Stent study continued. Complete a similar analysis for the stent data but this time for the one year data. In particular Read the data into your working directory. stent_study &lt;-read_csv(&#39;data/stent_study.csv&#39;) Complete similar steps as in the class notes. i. Use inspect on the data. ii. Create a table of outcome365 and group. Comment on the results. iii. Create a barchart of the data. inspect(stent_study) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## ## categorical variables: ## name class levels n missing ## 1 group character 2 451 0 ## 2 outcome30 character 2 451 0 ## 3 outcome365 character 2 451 0 ## distribution ## 1 control (50.3%), trmt (49.7%) ## 2 no_event (89.8%), stroke (10.2%) ## 3 no_event (83.8%), stroke (16.2%) tally(outcome365~group,data=stent_study,format=&quot;proportion&quot;,margins = TRUE) ## group ## outcome365 control trmt ## no_event 0.8766520 0.7991071 ## stroke 0.1233480 0.2008929 ## Total 1.0000000 1.0000000 Patients in the treatment group had a higher proportion of strokes than those in the control group after one year. The treatment does not appear to help the rate of strokes and in fact may hurt it. stent_study %&gt;% gf_props(~group,fill=~outcome365,position=&#39;fill&#39;) %&gt;% gf_labs(title=&quot;Impact of Stents of Stroke&quot;, subtitle=&#39;Experiment with 451 Patients&#39;, x=&quot;Experimental Group&quot;, y=&quot;Number of Events&quot;) 1.2.2 Problem 2 Migraine and acupuncture. A migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. 46 patients in the control group received placebo acupuncture (needle insertion at nonacupoint locations). 24 hours after patients received acupuncture, they were asked if they were pain free.1 The data is in the file migraine_study.csv in the folder data. Complete the following work: Read the data an object called migraine_study. migraine_study &lt;- read_csv(&quot;data/migraine_study.csv&quot;) head(migraine_study) ## # A tibble: 6 x 2 ## group pain_free ## &lt;chr&gt; &lt;chr&gt; ## 1 treatment yes ## 2 treatment yes ## 3 treatment yes ## 4 treatment yes ## 5 treatment yes ## 6 treatment yes Create a table of the data. tally(pain_free~group,data=migraine_study,format=&quot;proportion&quot;,margin=TRUE) ## group ## pain_free control treatment ## no 0.95652174 0.76744186 ## yes 0.04347826 0.23255814 ## Total 1.00000000 1.00000000 Report the percent of patients in the treatment group who were pain free 24 hours after receiving acupuncture. There are 23.2% of the treatment group pain free. Repeat for the control group. There are only 4.3% of the control group pain free. At first glance, does acupuncture appear to be an effective treatment for migraines? Explain your reasoning. Yes, a substantial increase in the percentage of patients pain free after acupuncture versus those with no acupuncture, so it appears to be effective. Do the data provide convincing evidence that there is a real pain reduction for those patients in the treatment group? Or do you think that the observed difference might just be due to chance? Either of these is acceptable: i. We could get slightly different group estimates even if there is no real difference. Though the difference is big, Im skeptical the results show a real difference and think this might be due to chance. ii. The difference in these rates looks pretty big, and so I suspect acupuncture is having a positive impact on pain. Compile, knit, this report into a pdf. Complete on your computer or server. G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173175. "],["DB.html", "Chapter 2 Data Basics 2.1 Objectives 2.2 Homework", " Chapter 2 Data Basics 2.1 Objectives Define and use properly in context all new terminology to include but not limited to case, observational unit, variables, data frame, associated variables, independent, and discrete and continuous variables. Identify and define the different types of variables. From reading a study, explain the research question. Create a scatterplot in R and determine the association of two numerical variables from the plot. 2.2 Homework Identify study components Identify (i) the cases, (ii) the variables and their types, and (iii) the main research question in the studies described below. 2.2.1 Problem 1 Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.2 The cases are 143,196 eligible study subjects who were born in Southern California between 1989 and 1993. The variables are measurements of carbon monoxide (CO), nitrogen dioxide, ozone, and particulate matter less than 10\\(\\mu m\\) (PM10) collected at air-quality-monitoring stations as well as length of gestation. All of these variables are continuous numerical variables. The research question was Is there an association between air pollution exposure and preterm births? 2.2.2 Problem 2 The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.3 The cases are 600 adult patients aged 18-69 years diagnosed and currently treated for asthma. The variables were whether or not the patient practiced the Buteyko method (categorical) and measures of quality of life, activity, asthma symptoms and medication reduction of the patients (categorical, ordinal). It may also be reasonable to treat the ratings on a scale of 1 to 10 as discrete numerical variables. The research question was Do asthmatic patients who practice the Buteyko method experience improvement in their condition? B. Ritz et al. Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993. In: Epidemiology 11.5 (2000), pp. 502511. J. McGowan. Health Education: Does the Buteyko Institute Method make a difference? In: Thorax 58 (2003). "],["ODCP.html", "Chapter 3 Overview of Data Collection Principles 3.1 Objectives 3.2 Homework", " Chapter 3 Overview of Data Collection Principles 3.1 Objectives Define and use properly in context all new terminology. From a description of a research project, at a minimum be able to describe the population of interest, the generalizability of the study, the response and predictor variables, differentiate whether it is observational or experimental, and determine the type of sample. 3.2 Homework 3.2.1 Problem 1 Generalizability and causality. Identify the population of interest and the sample in the studies described below, these are the same studies from the prevous lesson. Also comment on whether or not the results of the study can be generalized to the population and if the findings of the study can be used to establish causal relationships. Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.4 The population of interest is all births. The sample consists of the 143,196 births between 1989 and 1993 in Southern California. If births in this time span at the geography can be considered to be representative of all births, then the results are generalizable to the population of Southern California. However, since the study is observational the findings cannot be used to establish causal relationships. The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.5 The population is all 18-69 year olds diagnosed and currently treated for asthma. The sample is the 600 adult patients aged 18-69 years diagnosed and currently treated for asthma. Since the sample is not random (voluntary) the results cannot be generalized to the population at large. However, since the study is an experiment, the findings can be used to establish causal relationships. 3.2.2 Problem 2 GPA and study time. A survey was conducted on 55 undergraduates from Duke University who took an introductory statistics course in Spring 2012. Among many other questions, this survey asked them about their GPA and the number of hours they spent studying per week. The scatterplot below displays the relationship between these two variables. What is the explanatory variable and what is the response variable? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Is this an experiment or an observational study? Can we conclude that studying longer hours leads to higher GPAs? Solutions The explanatory variable is the number of study hours per week, and the response variable is GPA. There is a somewhat weak positive relationship between the two variables, though the data become more sparse as the number of study hours increases. One responded reported a GPA above 4.0, which is clearly a data error. Also, there are a few respondents who reported unusually high study hours (60 and 70 hours/week). It should also be noted that the variability in GPA is much higher for students who study less than those who study more, also might be due to the fact that there arent many respondents who reported studying higher hours. This is an observational study. Since this is an observational study, we cannot conclude that there is a causal relationship between the two variables even though there appears to be an association. 3.2.3 Problem 3 Income and education The scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelors degree in 3,143 counties in the US in 2010. What are the explanatory and response variables? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Can we conclude that having a bachelors degree increases ones income? Solutions The explanatory variable is percent of population with a bachelors degree and the response variable is per capita income (in thousands). There is a strong positive linear relationship between the two variables. As the percentage of population with a bachelors degree increases the per capita income increases as well. There are very few counties where more than 60% of the population have a bachelors degree and very few countries that have a more than $50,000 in per capita income. This is an observational study so we cannot make a causal statement based on the results. However, we can say that having a higher percentage of population with bachelors degree is associated with a higher per capita income. B. Ritz et al. Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993. In: Epidemiology 11.5 (2000), pp. 502511. J. McGowan. Health Education: Does the Buteyko Institute Method make a difference? In: Thorax 58 (2003). "],["STUDY.html", "Chapter 4 Studies 4.1 Objectives 4.2 Homework", " Chapter 4 Studies 4.1 Objectives Define and use properly in context all new terminology. Given a study description, be able to identify and explain the study using correct terms. Given a scenario, describe flaws in reasoning and propose study and sampling designs. 4.2 Homework 4.2.1 Problem 1 Propose a sampling strategy. A large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each of 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the students overall satisfaction with the course. What type of study is this? Observational study. Suggest a sampling strategy for carrying out this study. Stratified sample, sample randomly within each section. 4.2.2 Problem 2 Flawed reasoning. Identify the flaw in reasoning in the following scenarios. Explain what the individuals in the study should have done differently if they wanted to make such strong conclusions. Students at an elementary school are given a questionnaire that they are required to return after their parents have completed it. One of the questions asked is, Do you find that your work schedule makes it difficult for you to spend time with your kids after school? Of the parents who replied, 85% said no. Based on these results, the school officials conclude that a great majority of the parents have no difficulty spending time with their kids after school. Solution Non-responders may have a different response to this question. The parents who returned the surveys are probably those who do not have difficulty spending time with their kids after school. Parents who work might not have returned the surveys since they probably have a busier schedule. A survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. A follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. The researcher reports that these 567 women are representative of all mothers. Solution It is unlikely that the women who were reached at the same address 3 years later are a random sample. These missing responders are probably renters (as opposed to homeowners) which means that they might be in a lower socio-economic status than the respondents. 4.2.3 Problem 3 Sampling strategies. A Math 377 student who is curious about the relationship between the amount of time students spend on social networking sites and their performance at school decides to conduct a survey. Four research strategies for collecting data are described below. In each, name the sampling method proposed and any bias you might expect. He randomly samples 40 students from the studys population, gives them the survey, asks them to fill it out and bring it back the next day. He gives out the survey only to his friends, and makes sure each one of them fills out the survey. He posts a link to an online survey on his Facebook wall and asks his friends to fill out the survey. He stands outside the QRC and asks every third person that walks out the door to fill out the survey. Solution a. Simple random sample. Non-response bias, if only those people who have strong opinions about the survey responds his sample may not be representative of the population. b. Convenience sample. Under coverage bias, his sample may not be representative of the population since it consists only of his friends. It is also possible that the study will have non-response bias if some choose to not bring back the survey. c. Convenience sample. This will have a similar issues to handing out surveys to friends. d. Convenience sample. Same. 4.2.4 Problem 4 Vitamin supplements. In order to assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No significant differences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms. Was this an experiment or an observational study? Why? What are the explanatory and response variables in this study? Were the patients blinded to their treatment? Was this study double-blind? Participants are ultimately able to choose whether or not to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning. Solution a. Experiment, since the researchers randomly assigned different treatments to the participants. b. Response variable: Duration of the cold. Explanatory variable: Treatment, with 4 levels; placebo, 1g, 3g, 3g with additives. c. The patients were blinded as they did not know which treatment they received. d. The study was double-blind with respect to the researchers evaluating the patients, but the nurses who briely interacted with patients during the distribution of the medication were not blinded. (It was partially double-blind.) e. Since the patients were randomly assigned to the treatment groups and they are blinded we would expect about an equal number of patients in each group to not adhere to the treatment. While this means that final results of the study will be based on fewer number of participants, non-adherence does not introduce a confounding variable to the study. 4.2.5 Problem 5 Exercise and mental health. A researcher is interested in the effects of exercise on mental health and she proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results. What type of study is this? What are the treatment and control groups in this study? Does this study make use of blocking? If so, what is the blocking variable? Does this study make use of blinding? Comment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large. Suppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal? Solution a. This is an experiment since we assigned subjects to the exercise program. b. The treatment is exercise twice a week and control is no exercise. c, Yes, the blocking variable is age. d. No, the study is not blinded since the patients will know whether or not they are exercising. e. Since this is an experiment, we can make a causal statement. Since the sample is random, the causal statement can be generalized to the population at large. However, we should be cautious about making a causal statement because of a possible placebo effect. f. It would be very difficult, if not impossible, to successfully conduct this study since randomly sampled people cannot be required to participate in a clinical trial. "],["NUMDATA.html", "Chapter 5 Numerical Data 5.1 Objectives 5.2 Homework", " Chapter 5 Numerical Data 5.1 Objectives Define and use properly in context all new terminology. Generate in R summary statistics for a numeric variable including breaking down by cases. Generate in R appropriate graphical summaries of numerical variables. Be able to interpret and explain output both graphically and numerically. 5.2 Homework 5.2.1 Problem 1 Mammals exploratory Data were collected on 39 species of mammals distributed over 13 orders. The data is in the openintro package as mammals Using help, report the units for the variable BrainWt. ?mammals Using inspect how many variables are numeric? inspect(mammals) ## ## categorical variables: ## name class levels n missing ## 1 species factor 62 62 0 ## distribution ## 1 Africanelephant (1.6%) ... ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 body_wt numeric 0.005 0.600 3.3425 48.2025 6654.0 198.789984 ## ...2 brain_wt numeric 0.140 4.250 17.2500 166.0000 5712.0 283.134194 ## ...3 non_dreaming numeric 2.100 6.250 8.3500 11.0000 17.9 8.672917 ## ...4 dreaming numeric 0.000 0.900 1.8000 2.5500 6.6 1.972000 ## ...5 total_sleep numeric 2.600 8.050 10.4500 13.2000 19.9 10.532759 ## ...6 life_span numeric 2.000 6.625 15.1000 27.7500 100.0 19.877586 ## ...7 gestation numeric 12.000 35.750 79.0000 207.5000 645.0 142.353448 ## ...8 predation integer 1.000 2.000 3.0000 4.0000 5.0 2.870968 ## ...9 exposure integer 1.000 1.000 2.0000 4.0000 5.0 2.419355 ## ...10 danger integer 1.000 1.000 2.0000 4.0000 5.0 2.612903 ## sd n missing ## ...1 899.158011 62 0 ## ...2 930.278942 62 0 ## ...3 3.666452 48 14 ## ...4 1.442651 50 12 ## ...5 4.606760 58 4 ## ...6 18.206255 58 4 ## ...7 146.805039 58 4 ## ...8 1.476414 62 0 ## ...9 1.604792 62 0 ## ...10 1.441252 62 0 What type of variable is danger? Categorical Create a histogram of total_sleep and describe the distribution. gf_histogram(~total_sleep,data=mammals,binwidth = 2) gf_dens(~total_sleep,data=mammals) The distribution is unimodal and skewed to the right. It appears it is centered around the value of 11. Create a boxplot of life_span and describe the distribution. gf_boxplot(~life_span,data=mammals) Report the mean and median life span of a mammal. mean(~life_span,data=mammals,na.rm=TRUE) ## [1] 19.87759 median(~life_span,data=mammals,na.rm=TRUE) ## [1] 15.1 Calculate the summary statistics for LifeSpan broken down by Danger. favstats(life_span~danger,data=mammals) ## danger min Q1 median Q3 max mean sd n missing ## 1 1 3.0 7.700 17.60 32.500 100.0 24.20556 23.53829 18 1 ## 2 2 2.3 4.500 10.40 13.000 50.0 12.92308 13.15948 13 1 ## 3 3 2.0 4.175 5.35 7.875 38.6 9.43750 11.99559 8 2 ## 4 4 2.6 9.775 22.10 27.000 69.0 23.11000 18.75482 10 0 ## 5 5 17.0 20.000 23.60 30.000 46.0 26.95556 10.18910 9 0 5.2.2 Problem 2 Mammals life spans Continue using the mammals data set. Create side-by-side boxplots for life_span broken down by exposure. Note: you will have to change exposure to a factor(). Report on any findings. mammals %&gt;% gf_boxplot(life_span~factor(exposure)) What happened to the median and third quartile in exposure group 4? favstats(life_span~factor(exposure),data=mammals) ## factor(exposure) min Q1 median Q3 max mean sd n missing ## 1 1 2.0 4.35 7.25 14.550 100.0 14.55000 20.98594 24 3 ## 2 2 2.3 6.00 11.20 17.275 50.0 15.39167 14.55819 12 1 ## 3 3 7.6 19.90 26.50 32.000 41.0 25.40000 13.84582 4 0 ## 4 4 7.0 20.20 27.00 27.000 39.3 24.10000 11.78431 5 0 ## 5 5 16.3 20.00 28.00 38.600 69.0 30.53077 14.98084 13 0 Create faceted histograms. What are the shortcomings of this plot? gf_histogram(~life_span,color=~factor(exposure),data=mammals) This is awful. gf_histogram(~life_span|factor(exposure),data=mammals) Not enough data for each histogram; some of the histograms provide little to no information. Lets do denisty plots. gf_dens(~life_span,color=~factor(exposure),data=mammals) gf_dens(~life_span|factor(exposure),data=mammals) Which do you think is the best graph? Create a new variable exposed that is a factor with level Low if exposure is 1 or 2 and High otherwise. mammals &lt;- mammals %&gt;% mutate(exposed=factor(ifelse((exposure==1)|(exposure==2),&quot;Low&quot;,&quot;High&quot;))) inspect(mammals) ## ## categorical variables: ## name class levels n missing ## 1 species factor 62 62 0 ## 2 exposed factor 2 62 0 ## distribution ## 1 Africanelephant (1.6%) ... ## 2 Low (64.5%), High (35.5%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 body_wt numeric 0.005 0.600 3.3425 48.2025 6654.0 198.789984 ## ...2 brain_wt numeric 0.140 4.250 17.2500 166.0000 5712.0 283.134194 ## ...3 non_dreaming numeric 2.100 6.250 8.3500 11.0000 17.9 8.672917 ## ...4 dreaming numeric 0.000 0.900 1.8000 2.5500 6.6 1.972000 ## ...5 total_sleep numeric 2.600 8.050 10.4500 13.2000 19.9 10.532759 ## ...6 life_span numeric 2.000 6.625 15.1000 27.7500 100.0 19.877586 ## ...7 gestation numeric 12.000 35.750 79.0000 207.5000 645.0 142.353448 ## ...8 predation integer 1.000 2.000 3.0000 4.0000 5.0 2.870968 ## ...9 exposure integer 1.000 1.000 2.0000 4.0000 5.0 2.419355 ## ...10 danger integer 1.000 1.000 2.0000 4.0000 5.0 2.612903 ## sd n missing ## ...1 899.158011 62 0 ## ...2 930.278942 62 0 ## ...3 3.666452 48 14 ## ...4 1.442651 50 12 ## ...5 4.606760 58 4 ## ...6 18.206255 58 4 ## ...7 146.805039 58 4 ## ...8 1.476414 62 0 ## ...9 1.604792 62 0 ## ...10 1.441252 62 0 Repeat part c with the new variable. gf_dens(~life_span,color=~exposed,data=mammals) 5.2.3 Problem 3 Mammals life spans continued Create a scatterplot of life span versus length of gestation. mammals %&gt;% gf_point(life_span~gestation) What type of an association is apparent between life span and length of gestation? It is a weak positive association. What type of an association would you expect to see if the axes of the plot were reversed, i.e. if we plotted length of gestation versus life span? The same as this is observational data there is no reason to beliee is a causal relationship just by looking at the data. Switching the axis will preserve the association. Create the new scatterplot suggested in c. mammals %&gt;% gf_point(gestation~life_span) Are life span and length of gestation independent? Explain your reasoning. No there is an association and it appears to be linear. If the plot looked like a shotgun blast, we would consider the variables to be independent. However, remember there may be confounding variables that could impact the association between these variables. "],["CATDATA.html", "Chapter 6 Categorical Data 6.1 Objectives 6.2 Homework", " Chapter 6 Categorical Data 6.1 Objectives Define and use properly in context all new terminology. Generate in R tables for categorical variable(s). Generate in R appropriate graphical summaries of categorical and numerical variables. Be able to interpret and explain output both graphically and numerically. 6.2 Homework Make sure your plots have a title and the axes are labeled. 6.2.1 Problem 1 Views on immigration 910 randomly sampled registered voters from Tampa, FL were asked if they thought workers who have illegally entered the US should be (i) allowed to keep their jobs and apply for US citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for US citizenship, or (iii) lose their jobs and have to leave the country. The data is in the openintro package in the immigration data object. How many levels of political are there? levels(immigration$political) ## [1] &quot;conservative&quot; &quot;liberal&quot; &quot;moderate&quot; inspect(immigration) ## ## categorical variables: ## name class levels n missing ## 1 response factor 4 910 0 ## 2 political factor 3 910 0 ## distribution ## 1 Leave the country (38.5%) ... ## 2 conservative (40.9%), moderate (39.9%) ... There are three levels for political and they are conservative, liberal, and moderate. Create a table using tally. round(tally(~response+political,data=immigration,format=&quot;percent&quot;,margins = TRUE),2) ## political ## response conservative liberal moderate Total ## Apply for citizenship 6.26 11.10 13.19 30.55 ## Guest worker 13.30 3.08 12.42 28.79 ## Leave the country 19.67 4.95 13.85 38.46 ## Not sure 1.65 0.11 0.44 2.20 ## Total 40.88 19.23 39.89 100.00 What percent of these Tampa, FL voters identify themselves as conservatives? From the table, 40.88% of voters identified themselves as conservatives. What percent of these Tampa, FL voters are in favor of the citizenship option? Again, from the table 30.55% of the voters favor the citizenship option. What percent of these Tampa, FL voters identify themselves as conservatives and are in favor of the citizenship option? From the table, 6.26% of the voters are conservative and favor the citizenship option. What percent of these Tampa, FL voters who identify themselves as conservatives are also in favor of the citizenship option? What percent of moderates and liberal share this view? We need a different table for this question. round(tally(response~political,data=immigration,format=&quot;percent&quot;,margins = TRUE),2) ## political ## response conservative liberal moderate ## Apply for citizenship 15.32 57.71 33.06 ## Guest worker 32.53 16.00 31.13 ## Leave the country 48.12 25.71 34.71 ## Not sure 4.03 0.57 1.10 ## Total 100.00 100.00 100.00 Of the conservative voters, 15.32% are in favor of the citizenship option. The numbers are 57.71% for liberals and 33.06% for moderates. Create a stacked bar chart. immigration %&gt;% gf_props(~political,fill=~response,position=&quot;fill&quot;) %&gt;% gf_labs(title=&quot;Tampa Florida Voter Views on Illegal Immigrant Workers&quot;, subtitle=&quot;Broken down by political views&quot;,x=&quot;Political View&quot;,y=&quot;Proportion&quot;) %&gt;% gf_theme(theme_bw()) Using your plot, do political ideology and views on immigration appear to be independent? Explain your reasoning. The percentages of Tampa, FL conservatives, moderates, and liberals who are in favor of illegal immigrants working in the US staying and applying for citizenship are quite different from one another. Therefore, the two variables appear to be dependent. 6.2.2 Problem 2 Views on the DREAM Act The same survey from Exercise 1 also asked respondents if they support the DREAM Act, a proposed law which would provide a path to citizenship for people brought illegally to the US as children. The data is in the openintro package in the dream data object. Create a mosaic plot. mosaic(stance~ideology,data=dream,sub=&quot;Voter views on illegal worker status&quot;) Based on the mosaic plot, are views on the DREAM Act and political ideology independent? The vertical locations at which the ideological groups break into the Yes, No, and Not Sure categories differ, which indicates the variables are dependent. 6.2.3 Problem 3 Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable transplant indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called survived was used to indicate whether or not the patient was alive at the end of the study. The data is in the openintro package and is called heart_transplant. Create a mosaic plot. mosaic(survived~transplant,data=heart_transplant) Based on the mosaic plot, is survival independent of whether or not the patient got a transplant? Explain your reasoning. Proportion of patients who are alive at the end of the study is higher in the treatment group than in the control group. These data suggest that survival is not independent of whether or not the patient got a transplant. Using survtime create side-by-side boxplots for the control and treatment groups. heart_transplant %&gt;% gf_boxplot(survtime~transplant) %&gt;% gf_labs(title=&quot;Survival times for tranplant experiment&quot;, sub=&quot;Treatment group had the transplant&quot;,x=&quot;Tranplant&quot;,y=&quot;Survival time in days&quot;) %&gt;% gf_theme(theme_classic()) What do the box plots suggest about the efficacy (effectiveness) of transplants? The shape of the distribution of survival times in both groups is right skewed with one very clear outlier for the control group and other possible outliers in both groups on the high end. The median survival time for the control group is much lower than the median survival time for the treatment group; patients who got a transplant typically lived longer. Tying this together with the much lower variability in the control group, evident by a much smaller IQR than the treatment group (about 50 days versus 500 days), and we can see that patients who did not get a heart transplant tended to consistently die quite early relative to those who did have a transplant. Overall, very few patients without transplants made it beyond a year while nearly half of the transplant patients survived at least one year. It should also be noted that while the first and third quartiles of the treatment group is higher than those for the control group, the IQR for the treatment group is much bigger, indicating that there is more variability in survival times in the treatment group. "],["CS2.html", "Chapter 7 Case Study 7.1 Objectives 7.2 Homework", " Chapter 7 Case Study 7.1 Objectives Use R to simulate a probabilistic model. Use basic counting methods. 7.2 Homework 7.2.1 Problem 1 Exactly 2 people with the same birthday - Simulation. Complete a similar analysis for case where exactly 2 people in a room of 23 people have the same birthday. In this exercise you will use a computational simulation. Create a new R Markdown file and create a report. Yes, we know you could use this file but we want you to practice generating your own report. Simulate having 23 people in the class with each day of the year equally likely. Find the cases where exactly 2 people have the same birthday, you will have to alter the code from the Notes more than changing 18 to 23. Plot the frequency of occurrences as a bar chart. Estimate the probability of exactly two people having the same birthday. (do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% mutate(match=if_else(length==22,1,0)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.3596 (do(1000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% gf_bar(~length) 7.2.2 Problem 2 Exactly 2 people with the same birthday - Mathematical. Repeat problem 1 but do it mathematically. As a big hint, you will need to use the choose() function. The idea is that with 23 people we need to choose 2 of them to match. We thus need to multiply, the multiplication rule again, by choose(23,2). If you are having trouble, work with a total of 3 people in the room first. Find a formula to determine the exact probability of exactly 2 people in a room of 23 having the same birthday. Generalize your solution to any number n people in the room and create a function. Vectorize the function. Plot the probability of exactly 2 people having the same birthday versus number of people in the room. Comment on the shape of the curve and explain it. knit and compile your report. For two people we have choose(23,2)*prod(365:344)/365^23 ## [1] 0.3634222 exactly_two &lt;- function(n){ choose(n,2)*prod(365:(365-(n-2)))/365^n } exactly_two(23) ## [1] 0.3634222 exactly_two &lt;- Vectorize(exactly_two) gf_line(exactly_two(1:100)~ seq(1,100), xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of exactly least 2 people with matching birthdays&quot;) By the way, exactly three matches in simulation is hard. We have to table the data set.seed(10) temp &lt;- table(sample(days,size=23,replace = TRUE)) temp ## ## 13 24 50 72 92 110 137 143 154 155 211 231 263 271 285 330 338 342 344 351 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 ## 365 ## 1 (sum(temp==2) == 2)+0 ## [1] 1 (do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 3)==1)+0)) %&gt;% summarise(prob=mean(result)) ## prob ## 1 0.0117 Two sets that have same but different birthday (do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 2)==2)+0)) %&gt;% summarise(prob=mean(result)) ## prob ## 1 0.1139 (do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% mutate(match=if_else(length==21,1,0)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.1187 Mathematically exactly 3 is easy. Simulation seems to be off a little or the math formula is off. choose(23,3)*prod(365:345)/365^23 ## [1] 0.007395218 "],["PROBRULES.html", "Chapter 8 Probability Rules 8.1 Objectives 8.2 Homework", " Chapter 8 Probability Rules 8.1 Objectives Define and use properly in context all new terminology related to probability to include but not limited to: outcome, event, sample space, probability. Apply basic probability and counting rules to find probabilities. Describe the basic axioms of probability. Use R to calculate and simulate probabilities of events. 8.2 Homework 8.2.1 Problem 1 1. Let \\(A\\), \\(B\\) and \\(C\\) be events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), and \\(\\mbox{P}(C)=0.4\\). Also, we know that \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Find the following: \\(\\mbox{P}(A\\cup B)\\) \\[ \\mbox{P}(A\\cup B) = \\mbox{P}(A)+\\mbox{P}(B)-\\mbox{P}(A\\cap B)= 0.5+0.3-0.2 = 0.6 \\] \\(\\mbox{P}(A\\cup B \\cup C)\\) \\[ \\mbox{P}(A\\cup B \\cup C) = \\mbox{P}(A)+\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(A\\cap B)-\\mbox{P}(A\\cap C)-\\mbox{P}(B\\cap C)+\\mbox{P}(A\\cap B \\cap C) \\] \\[ = 0.5+0.3+0.4-0.2-0.12-0.1+0.05 = 0.83 \\] \\(\\mbox{P}(B&#39;\\cap C&#39;)\\) \\[ \\mbox{P}(B&#39;\\cap C&#39;)=\\mbox{P}((B\\cup C)&#39;) = 1-\\mbox{P}(B\\cup C) = 1-[\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(B\\cap C)] \\] \\[ = 1-(0.3+0.4-0.12) = 0.42 \\] \\(\\mbox{P}(A\\cup (B\\cap C))\\) \\[ \\mbox{P}(A\\cup (B\\cap C)) = \\mbox{P}(A)+\\mbox{P}(B\\cap C) -\\mbox{P}(A\\cap B \\cap C) = 0.5+0.12-0.05 = 0.57 \\] \\(\\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)\\) \\[ \\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)=\\mbox{P}(A\\cup B \\cup C)-\\mbox{P}(A\\cap B \\cap C) = 0.83-0.05 = 0.78 \\] 8.2.2 Problem 2 2. Consider the example of the family in the reading. What is the probability that the family has at least one boy? \\[ \\mbox{P}(\\mbox{at least one boy})=1-\\mbox{P}(\\mbox{no boys})=1-\\mbox{P}(\\mbox{GGG})=1-\\frac{1}{8} = 0.875 \\] 8.2.3 Problem 3 3. The Birthday Problem Revisited. Suppose there are \\(n=20\\) students in a classroom. My birthday, the instructor, is April 3rd. What is the probability that at least one student shares my birthday? Assume only 365 days in a year and assume that all birthdays are equally likely. \\[ \\mbox{P}(\\mbox{at least one other person shares my bday})=1-\\mbox{P}(\\mbox{no one else has my bday}) = \\] \\[ 1-\\left( \\frac{364}{365}\\right)^{20} = 0.0534 \\] In R, find the probability that at least one other person shares my birthday for each value of \\(n\\) from 1 to 80. Plot these probabilities with \\(n\\) on the \\(x\\)-axis and probability on the \\(y\\)-axis. At what value of \\(n\\) would the probability be at least 50%? Generalizing, \\[ \\mbox{P}(\\mbox{at least one other person shares my bday})=1-\\mbox{P}(\\mbox{no one else has my bday}) = 1-\\left( \\frac{364}{365}\\right)^{n} \\] n&lt;-1:300 mybday&lt;-function(x) 1-(364/365)^x mybday &lt;- Vectorize(mybday) Check our function. mybday(20) ## [1] 0.05339153 gf_line(mybday(n)~ n, xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of at least 1 person matching my birthday&quot;) %&gt;% gf_theme(theme_bw) prob &lt;- mybday(n) which(prob&gt;= .5) ## [1] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 ## [20] 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 ## [39] 291 292 293 294 295 296 297 298 299 300 So 253 people. 8.2.4 Problem 4 Thinking of the cards again. Answer the following questions: Define two events that are mutually exclusive. The first card drawn is red. The first card drawn is black. Define two events that are independent. The first card drawn is black. The first card drawn is a face card. Define an event and its complement. The first card drawn is less than 5. The first card drawn is equal to or more than 5. 8.2.5 Problem 5 Consider the license plate example from the reading. What is the probability that a license plate contains exactly one B? #fourth spot num4&lt;-10*10*10*1*25*25 #fifth spot num5&lt;-10*10*10*25*1*25 #sixth spot num6&lt;-10*10*10*25*25*1 denom&lt;-10*10*10*26*26*26 (num4+num5+num6)/denom ## [1] 0.1066796 What is the probability that a license plate contains at least one B? \\[ 1-\\mbox{P}(\\mbox{no B&#39;s}) \\] num0&lt;-10*10*10*25*25*25 1-num0/denom ## [1] 0.1110036 8.2.6 Problem 6 Consider the party example in the reading. Suppose 8 people showed up to the party dressed as zombies. What is the probability that all three awards are won by people dressed as zombies? \\[ \\frac{8\\cdot 7 \\cdot 6}{25\\cdot 24 \\cdot 23} \\] (8*7*6)/(25*24*23) ## [1] 0.02434783 What is the probability that zombies win most creative and funniest but not scariest? \\[ \\frac{8 \\cdot 17 \\cdot 7}{25 \\cdot 24 \\cdot 23} \\] (8*17*7)/(25*24*23) ## [1] 0.06898551 8.2.7 Problem 7 Consider the cards example from the reading. How many ways can we obtain a two pairs (2 of one number, 2 of another, and the final different)? We have to pick the rank of the two pairs. \\[\\binom{13}{2}\\] Notice here the order does matter because a pair of Kings and 4s is the same as a pair of 4s and Kings. This is different from the full house example. Make sure you understand this point. Now we have to pick two of the fours cards for each rank \\[\\binom{4}{2}\\binom{4}{2}\\] And finally we need the last card to come from the 44 remaining cards so that we dont get a full house. \\(\\binom{44}{1}\\) Putting it all together: \\(\\binom{13}{2}\\binom{4}{2}\\binom{4}{2}\\binom{44}{1}\\) choose(13,2)*choose(4,2)*choose(4,2)*choose(44,1) ## [1] 123552 What is the probability of drawing a four of a kind (four cards of the same value)? \\[ \\mbox{P}(\\mbox{4 of a kind})=\\frac{\\binom{13}{1}\\binom{4}{4}\\binom{48}{1}}{\\binom{52}{5}} \\] (13*1*48)/choose(52,5) ## [1] 0.000240096 8.2.8 Problem 8 Advanced Question: Consider rolling 5 dice. What is the probability of a pour resulting in a full house? First pick the value for the three of a kind, there are 6. Then pick the value from the remaining 5 for the two of a kind. This is actually a permutation. There are 30 distinct flavors of full house (three 1s &amp; two 2s, three 1s &amp; two 3s, etc.). In the reading we did this as \\[ \\binom{6}{1} \\times \\binom{5}{1} \\] We now have the 5 dice. We have to select three to have the same value and the order doesnt matter since they are the same value. Thus we multiple by \\(\\binom{5}{3}\\). Divide this by the total distinct ways the dice could have landed (assuming order matters). \\[ \\mbox{P}(\\mbox{full house}) = \\frac{30 \\times \\frac{5!}{3!2!}}{6^5} \\] \\[ \\mbox{P}(\\mbox{full house}) = \\frac{\\binom{6}{1} \\times \\binom{5}{1} \\times \\binom{5}{3}}{6^5} \\] 30*10/(6^5) ## [1] 0.03858025 Simulating is tough so lets write some code that may help. set.seed(23) temp&lt;-table(sample(1:6,size=5,replace=TRUE)) temp ## ## 1 3 4 5 ## 1 2 1 1 sum(temp==2) &amp; sum(temp==3) ## [1] FALSE temp&lt;-c(1,1,1,2,2) temp&lt;-table(temp) temp ## temp ## 1 2 ## 3 2 sum(temp==2) &amp; sum(temp==3) ## [1] TRUE Lets write a function. full_house &lt;-function(x){ temp&lt;-table(x) sum(temp==2) &amp; sum(temp==3) } temp&lt;-c(1,1,1,2,2) full_house(temp) ## [1] TRUE set.seed(751) results&lt;-do(10000)*full_house(sample(1:6,size=5,replace=TRUE)) mean(~full_house,data=results) ## [1] 0.039 "],["CONDPROB.html", "Chapter 9 Conditional Probability 9.1 Objectives 9.2 Homework", " Chapter 9 Conditional Probability 9.1 Objectives Define conditional probability and distinguish it from joint probability. Find a conditional probability using its definition. Using conditional probability, determine whether two events are independent. Apply Bayes Rule mathematically and via simulation. 9.2 Homework 9.2.1 Problem 1 1. Consider Exercise 1 from Lesson 2. Recall: \\(A\\), \\(B\\) and \\(C\\) are events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Are \\(A\\) and \\(B\\) independent? No. \\(\\mbox{P}(A)\\mbox{P}(B)=0.15\\neq \\mbox{P}(A\\cap B)\\). Are \\(B\\) and \\(C\\) independent? Yes. \\(\\mbox{P}(B)\\mbox{P}(C)=0.12 = \\mbox{P}(B\\cap C)\\). Also, \\[ \\mbox{P}(B|C)=\\frac{\\mbox{P}(B\\cap C)}{\\mbox{P}(C)}= 0.12/0.4 = 0.3 =\\mbox{P}(B) \\] 9.2.2 Problem 2 2. Suppose I have a biased coin (the probability I flip a heads is 0.6). I flip that coin twice. Assume that the coin is memoryless (flips are independent of one another). What is the probability that the second flip results in heads? 0.6 What is the probability that the second flip results in heads, given the first also resulted in heads? The coin is memoryless. So, \\[ \\mbox{P}(\\mbox{2nd flip heads}|\\mbox{1st flip heads}) = 0.6 \\] What is the probability both flips result in heads? Since the flips are independent, \\[ \\mbox{P}(\\mbox{both heads})=\\mbox{P}(\\mbox{1st flip heads})\\mbox{P}(\\mbox{2nd flip heads}) = 0.6*0.6=0.36 \\] What is the probability exactly one coin flip results in heads? This could happen in two ways. The first could be heads OR the second could be heads. \\[ \\mbox{P}(\\mbox{exactly one heads})=\\mbox{P}(\\mbox{1st flip heads})\\mbox{P}(\\mbox{2nd flip tails}) + \\mbox{P}(\\mbox{1st flip tails})\\mbox{P}(\\mbox{2nd flip heads}) \\] \\[ 0.6*0.4+0.4*0.6 = 0.48 \\] Now assume I flip the coin five times. What is the probability the result is 5 heads? \\[ \\mbox{P}(\\mbox{5 heads})= 0.6^5 = 0.0778 \\] 0.6^5 ## [1] 0.07776 What is the probability the result is exactly 2 heads (out of 5 flips)? There are \\(\\binom{5}{2} = 10\\) ways for this to happen ({HHTTT},{HTHTT},). So, \\[ \\mbox{P}(\\mbox{2 heads out of 5 flips})=\\binom{5}{2} 0.6^2(1-0.6)^3 = 0.2304 \\] choose(5,2)*0.6^2*0.4^3 ## [1] 0.2304 9.2.3 Problem 3 3. (Adapted from IPSUR, (Kerns 2010)). Suppose there are three assistants working at a company: Moe, Larry and Curly. All three assist with a filing process. Only one filing assistant is needed at a time. Moe assists 60% of the time, Larry assists 30% of the time and Curly assists the remaining 10% of the time. Occasionally, they make errors (misfiles); Moe has a misfile rate of 0.01, Larry has a misfile rate of 0.025, and Curly has a rate of 0.05. Suppose a misfile was discovered, but it is unknown who was on schedule when it occurred. Who is most likely to have committed the misfile? Calculate the probabilities for each of the three assistants. Let \\(E\\) be the event a misfile was committed. Also, let \\(M\\), \\(L\\), and \\(C\\) denote the events that Moe, Larry and Curly was the assistant at the time, respectively. \\[ \\mbox{P}(E)=\\mbox{P}(E \\cap M)+\\mbox{P}(E \\cap L)+\\mbox{P}(E\\cap C) \\] \\[ = \\mbox{P}(E|M)\\mbox{P}(M)+\\mbox{P}(E|L)\\mbox{P}(L)+\\mbox{P}(E|C)\\mbox{P}(C) = 0.01*0.6+0.025*0.3+0.05*0.1 = 0.0185 \\] Thus, \\[ \\mbox{P}(M|E)=\\frac{\\mbox{P}(E \\cap M)}{\\mbox{P}(E)}= \\frac{0.01*0.6}{0.0185}=0.3243 \\] Similarly, \\(\\mbox{P}(L|E)=0.4054\\) and \\(\\mbox{P}(C|E)=0.2702\\). Larry is the assistant most likely to have committed the error. 9.2.4 Problem 4 4. You are playing a game where there are two coins. One coin is fair and the other comes up heads 80% of the time. One coin is flipped 3 times and the result is three heads, what is the probability that the coin flipped is the fair coin? You will need to make an assumption about the probability of either coin being selected. Use Bayes formula to solve this problem. I will assume either coin is selected with a 50% probability. \\[ \\mbox{P}(Fair) = \\mbox{P}(Biased) = .5 \\] \\[ \\mbox{P}(3 Heads|Fair)=\\frac{1}{2}^3=\\frac{1}{8} \\] \\[ \\mbox{P}(3 Heads|Biased)=.8^3=0.512 \\] Now \\[ \\mbox{P}(Fair | 3 Heads) = \\frac{\\mbox{P}(3 Heads | Fair)\\mbox{P}(Fair)}{\\mbox{P}(3 Heads | Fair)\\mbox{P}(Fair)+\\mbox{P}(3 Heads| Biased)\\mbox{P}(Biased)} \\] Which is \\[ \\mbox{P}(Fair | 3 Heads) = \\frac{\\frac{1}{8}\\frac{1}{2}}{\\frac{1}{8}\\frac{1}{2}+.8^{3}\\frac{1}{2}} = 0.196 \\] .125*.5/(.125*.5+.8^3*.5) ## [1] 0.1962323 Use simulation to solve this problem. Lets use the same assumptions. We could do this problem in two ways. We could flip each coin a fixed number of times and combine the information or use a random process to pick a flipped coin and then flip it three times. Lets do the first. Lets flip a fair coin 50,000 times and count how many heads we get. set.seed(1154) data.frame(do(50000)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 6157 Now flip the biased coin. data.frame(do(50000)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 25743 So we have a total 6157 + 25743 heads of which 6157 came from the fair coin. Thus the probability of the coin being fair given 3 heads on the flips is: 6157/(6157 + 25743) ## [1] 0.1930094 Or 19.3%. Next pick a one of the coins with equal probability 100,000 times. set.seed(501) results &lt;- rflip(100000,summarize = TRUE) results ## n heads tails prob ## 1 1e+05 50226 49774 0.5 Now the fair coin was flipped 50226 times. Lets see how many times we get 3 heads when we flip that coin 3 times. data.frame(do(50226)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 6270 We have 6270 cases with 3 heads. Now for the biased coin. data.frame(do(49774)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 25512 Now we can determine the probability of a fair coin given 3 heads. 6270/(6270+25512) ## [1] 0.1972815 This code we could easily adapt if we dont think each coin is being selected with the same frequency. Suppose we think the fair coin has a 75% chance of being selected. The analysis would look like this: set.seed(9021) results &lt;- rflip(100000,prob=.75,summarize = TRUE) results ## n heads tails prob ## 1 1e+05 75023 24977 0.75 Now the fair coin was flipped 75023 times. Lets see how many times we get 3 heads when we flip that coin 3 times. data.frame(do(75023)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 9579 We have 9579 cases with 3 heads. Now for the biased coin. data.frame(do(24977)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 12789 Now we can determine the probability of a fair coin given 3 heads. 9579/(9579+12789) ## [1] 0.4282457 A much different answer. That is because prior to getting the data we believed the fair coin would be selected with a 75% probability. The data indicates that we need to update and lower this probability. We only flipped 3 times but the evidence is so in favor of the biased coin, that our probability dropped substantially. This is why Bayes is such a powerful tool. Think about what we just did with this problem. We started with a subjective believe that either coin would be selected with equal probability. This is called the prior probability. We then collected data on three flips of the coin. We used this empirical data to update our belief into a posterior probability. This is the basis for Bayesian statistical analysis. Bayesian statistics is an entire discipline unto itself. References "],["RANDVAR.html", "Chapter 10 Random Variables 10.1 Objectives 10.2 Homework", " Chapter 10 Random Variables 10.1 Objectives Define and use properly in context all new terminology. Given a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events. Simulate random variable for a discrete distribution. Find the moments of a discrete random variable. Find the expected value of a linear transformation of a random variable. 10.2 Homework 10.2.1 Problem 1 1. Suppose we are flipping a fair coin, and the result of a single coin flip is either heads or tails. Let \\(X\\) be a random variable representing the number of flips until the first heads. Is \\(X\\) discrete or continuous? What is the domain/support of \\(X\\)? \\(X\\) is discrete since number of flips is a discrete process (I cant perform a fraction of a flip). The wording is specific in that it is the number of flips until the first heads, so we must flip at least once. The domain of \\(X\\) is \\(S_X=\\{1,2,...\\}\\). What values do you expect \\(X\\) to take? What do you think is the average of \\(X\\)? Dont actually do any formal math, just think about if you were flipping a regular coin, how long it would take you to get the first heads. I would expect \\(X\\) to be 1 or 2 fairly often, since the coin is fair and has an even chance of landing on heads or tails. I would expect large values of \\(X\\) to be rare. For these reasons, I think the average of \\(X\\) should be around 2 flips or a little less than 2. Advanced: In R, generate 10,000 observations from \\(X\\). What is the average value of \\(X\\) based on this simulation? Note: There are many ways to do this. Below is a description of one approach. set.seed(68) which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[1] ## [1] 2 Now repeat using replicate() or do(). We will repeat 10000 times. results &lt;- do(10000)*which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[1] mean(~result,data=results) ## [1] 1.9849 tally(~result,data=results,format=&quot;percent&quot;) ## result ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 49.89 25.35 12.33 6.56 3.25 1.27 0.66 0.39 0.12 0.07 0.06 0.03 0.02 results %&gt;% gf_props(~result,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of flips&quot;, subtitle=&quot;Number of flips until first heads&quot;) As predicted, the mean is close to 2, and the most common values of \\(X\\) are 1 and 2. The most common is 1 occurring 50% of the time, this is what we would think since the coin comes up Heads 50% of the time. We know that \\(\\mbox{P}(X=1) = \\frac{1}{2}\\) and \\(\\mbox{P}(X=2) = \\frac{1}{2^2}\\) so in general \\(\\mbox{P}(X=x) = \\frac{1}{2^x}\\). This is the pmf. As an extra, to show that the sum of the infinite sequence of probabilities is 1 requires some Calculus knowledge. Lets start with a partial sum: \\[S_n=\\frac{1}{2}+\\frac{1}{4} +\\cdots + \\frac{1}{2^n}\\] Now multiply both sides by \\(\\frac{1}{2}\\). \\[\\frac{1}{2}S_n=\\frac{1}{4}+\\frac{1}{8} +\\cdots + \\frac{1}{2^{n+1}}\\] The difference between these two sums is \\[S_n-\\frac{1}{2}S_n=\\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}}\\] Now as \\[\\lim_{n \\to +\\infty} \\frac{1}{2^{n+1}} = 0\\] So \\[\\lim_{n \\to +\\infty} \\left[ \\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}} \\right]\\] This implies that \\(S = 1\\). 10.2.2 Problem 2 2. Repeat Problem 1, except part d, but with a different random variable, \\(Y\\): the number of coin flips until the fifth heads. \\(Y\\) is discrete for the same reasons as \\(X\\). The domain of \\(Y\\) is \\(S_Y=\\{5,6,...\\}\\). In order to land on heads five times, it would be reasonable to expect around 9 to 13 flips. Thus, I would expect \\(Y\\) to take values 8, 9, 10, 11, and 12 fairly often, and values outside of that range less often. I think the average of \\(Y\\) should be around 10 or so. set.seed(102) results &lt;- do(10000)*which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[5] mean(~result,data=results) ## [1] 9.9728 tally(~result,data=results,format=&quot;percent&quot;) ## result ## 5 6 7 8 9 10 11 12 13 14 15 16 17 ## 3.06 8.21 12.14 13.26 13.71 11.52 10.74 8.17 6.06 4.50 3.02 1.86 1.32 ## 18 19 20 21 22 23 24 25 28 29 ## 0.88 0.65 0.31 0.21 0.16 0.12 0.04 0.04 0.01 0.01 results %&gt;% gf_props(~result,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of flips&quot;, subtitle=&quot;Number of flips until 5th heads&quot;) The most common values of \\(Y\\) are between 6 and 11. The average of \\(Y\\) in this simulation is 9.97, close to what we predicted. The pmf is not that bad but you must know about the binomial distribution first. If we get the fifth heads on the nth flip, the prior n-1 flips are a binomial with n-1 successes. The final flip is a success so we multiply the binomial by the probability of success. 10.2.3 Problem 3 3. Suppose you are a data analyst for a large international airport. Your boss, the head of the airport, is dismayed that this airport has received negative attention in the press for inefficiencies and sluggishness. In a staff meeting, your boss gives you a week to build a report addressing the timeliness at the airport. Your boss is in a big hurry and gives you no further information or guidance on this task. Prior to building the report, you will need to conduct some analysis. To aid you in this, create a list of at least three random variables that will help you address timeliness at the airport. For each of your random variables, Determine whether it is discrete or continuous. Report its domain. What is the experimental unit? Explain how this random variable will be useful in addressing timeliness at the airport. I will provide one example: Let \\(D\\) be the difference between a flights actual departure and its scheduled departure. This is a continuous random variable, since time can be measured in fractions of minutes. A flight can be early or late, so domain is any real number. The experimental unit is each individual (non-canceled) flight. This is a useful random variable because the average value of \\(D\\) will describe whether flights take off on time. We could also find out how often \\(D\\) exceeds 0 (implying late departure) or how often \\(D\\) exceeds 30 minutes, which could indicate a very late departure. There are many correct answers. \\(X\\): Time it takes for a passenger to go through security (defined as time from entering security line to departing security with all belongings). Continuous. Experimental unit is individual passenger. This variable would help identify whether security line is too long. We could also explore how \\(X\\) changes based on day or time of day. \\(Y\\): Status of each scheduled departure (on time, somewhat late, very late, canceled). Discrete. Experimental unit is each scheduled departure. This variable will help describe how often flights are canceled or late. We could also explore \\(Y\\) by airline, destination, time of day, etc. \\(Z\\): Number of time-related complaints at customer service desk in a given day. Discrete. Experimental unit is day. This variable will describe attitudes/perceptions of customers. It is probably a bad sign if customers feel like the airport is not working efficiently. We can explore how \\(Z\\) changes over time. 10.2.4 Problem 4 4. Consider the experiment of rolling two fair six-sided dice. Let the random variable \\(Y\\) be the absolute difference between the two numbers that appear upon rolling the dice. What is the domain/support of \\(Y\\)? \\(S_Y=\\{0,1,2,3,4,5\\}\\). What values do you expect \\(Y\\) to take? What do you think is the average of \\(Y\\)? Dont actually do any formal math, just think about the experiment. Id say that \\(Y\\) should take values 0,1 and 2 fairly often. Id guess that the average should be around 1.5. Find the probability mass function and cumulative distribution function of \\(Y\\). Using counting methods, we know there are 36 possible values. We can just count them. The number 0 will occur when both numbers are the same, which happens six times. The number 1 happens when the first die is one larger than the second, 5 times, or vice versa. Thus 1 happens 10 times. Continue this process. Thus, the pmf of \\(Y\\) becomes: \\[ f_Y(y)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{6}{36}, &amp; y=0 \\\\ \\frac{10}{36}, &amp; y=1 \\\\ \\frac{8}{36}, &amp; y=2 \\\\ \\frac{6}{36}, &amp; y=3 \\\\ \\frac{4}{36}, &amp; y=4 \\\\ \\frac{2}{36}, &amp; y=5 \\\\ 0, &amp; \\mbox{otherwise} \\end{array} \\right . \\] We could also create a table and count the entries. \\[ \\begin{array}{cc|cccccc} &amp; &amp; &amp; &amp;\\textbf{Die} &amp; \\textbf{2} \\\\ &amp; &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\&amp;\\hline 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\\\textbf{Die 1} &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 \\\\&amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\\\&amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 \\\\&amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 \\\\&amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 \\end{array} \\] The cdf of \\(Y\\) is thus, \\[ F_Y(y)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, &amp; y &lt; 0 \\\\ \\frac{6}{36}, &amp; 0\\leq y &lt;1 \\\\ \\frac{16}{36}, &amp; 1\\leq y &lt;2 \\\\ \\frac{24}{36}, &amp; 2 \\leq y &lt;3 \\\\ \\frac{30}{36}, &amp; 3 \\leq y &lt;4 \\\\ \\frac{34}{36}, &amp; 4 \\leq y &lt;5 \\\\ \\frac{36}{36}, &amp; y\\geq 5 \\end{array} \\right . \\] Find the expected value and variance of \\(Y\\). \\[ \\mbox{E}(Y)=\\sum_{y=0}^5 y\\mbox{P}(Y=y) = 0\\times {6\\over 36} + 1 \\times {10\\over 36} + 2\\times {8\\over 36} + 3\\times {6\\over 36} + 4 \\times {4\\over 36} + 5 \\times {2\\over 36} = \\] \\[ {70\\over 36} = 1.944 \\] y&lt;-c(0,1,2,3,4,5) mean_y&lt;-sum(y*c(6,10,8,6,4,2)/36) mean_y ## [1] 1.944444 The variance is: sum((y-mean_y)^2*(c(6,10,8,6,4,2)/36)) ## [1] 2.052469 Advanced: In R, obtain 10,000 realizations of \\(Y\\). In other words, simulate the roll of two fair dice, record the absolute difference and repeat this 10,000 times. Construct a frequency table of your results (what percentage of time did you get a difference of 0? difference of 1? etc.) Find the mean and variance of your simulated sample of \\(Y\\). Were they close to your answers in part d? set.seed(9) sim_diffs&lt;-do(10000)*abs(diff(sample(1:6,2,replace=T))) tally(~abs,data=sim_diffs,format=&quot;proportion&quot;) ## abs ## 0 1 2 3 4 5 ## 0.1643 0.2752 0.2273 0.1618 0.1116 0.0598 mean(~abs,data=sim_diffs) ## [1] 1.9606 var(sim_diffs)*9999/10000 ## abs ## abs 2.077248 true_mean&lt;-sum(c(6,10,8,6,4,2)/36*c(0,1,2,3,4,5)) true_mean ## [1] 1.944444 sum(c(6,10,8,6,4,2)/36*(c(0,1,2,3,4,5)-true_mean)^2) ## [1] 2.052469 We got similar mean and variance to the theoretical values. 10.2.5 Problem 5 5. Prove the Lemma from the Notes: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Show \\(\\mbox{E}(aX + b)=a\\mbox{E}(X)+b\\). \\[ \\mbox{E}(aX+b)=\\sum_x (ax+b)f_X(x) = \\sum_x axf_X(x)+\\sum_x bf_X(x) + a\\sum_x xf_X(x)+b\\sum_x f_X(x) \\] Since \\(\\sum_x xf_X(x) = \\mbox{E}(X)\\) and \\(\\sum_x f_X(x)=1\\), this reduces to \\(a\\mbox{E}(X)+b\\). \\[ \\mbox{Var}(aX+b)=\\mbox{E}\\left[(aX+b-\\mbox{E}(aX+b))^2\\right]=\\mbox{E}\\left[(aX+b-a\\mbox{E}(X)-b)^2\\right]=\\mbox{E}\\left[(aX-a\\mbox{E}(X)^2\\right] \\] \\[ =\\mbox{E}\\left[a^2(X-\\mbox{E}(X))^2\\right]=a^2\\mbox{E}\\left[(X-\\mbox{E}(X))^2\\right]=a^2\\mbox{Var}(X) \\] 10.2.6 Problem 6 6. In the Notes, we saw that \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show that \\(\\mbox{Var}(X)\\) is also equal to \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\). \\[ \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]=\\mbox{E}[X^2-2\\mu_XX+\\mu_X^2] = \\mbox{E}(X^2)-\\mbox{E}(2\\mu_XX)+\\mbox{E}(\\mu_X^2) \\] The quantity \\(\\mu_X\\) is a constant with respect to \\(X\\), so \\[ =\\mbox{E}(X^2)-2\\mu_X\\mbox{E}(X)+\\mu_X^2=\\mbox{E}(X^2)-2\\mu_X^2+\\mu_X^2 = \\mbox{E}(X^2)-\\mu_X^2 \\] "],["CONRANDVAR.html", "Chapter 11 Continuous Random Variables 11.1 Objectives 11.2 Homework", " Chapter 11 Continuous Random Variables 11.1 Objectives Define and properly use the new terms to include probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables. Given a continuous random variable, find probabilities using the pdf and/or the cdf. Find the mean and variance of a continuous random variable. 11.2 Homework 11.2.1 Problem 1 1. Let \\(X\\) be a continuous random variable on the domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\). Assume that \\(f(x)\\) is a valid pdf. Find the value of \\(k\\). Because \\(f\\) is a valid pdf, we know that \\(\\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = 1\\). So, \\[ \\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^3}{54}\\bigg|_{-k}^k = \\frac{k^3}{54}-\\frac{-k^3}{54}=\\frac{k^3}{27}=1 \\] Thus, \\(k=3\\). Using R, see if you can follow the code. my_pdf &lt;- function(x)integrate(function(y)y^2/18,-x,x)$value my_pdf&lt;-Vectorize(my_pdf) domain &lt;- seq(.01,5,.1) gf_line(my_pdf(domain)~domain) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Cumulative probability for different values of k&quot;,x=&quot;k&quot;,y=&quot;Cummulative Probability&quot;) %&gt;% gf_hline(yintercept = 1,color = &quot;blue&quot;) Looks like \\(k \\approx 3\\) from the plot. uniroot(function(x)my_pdf(x)-1,c(-10,10))$root ## [1] 2.999997 Plot the pdf of \\(X\\). x&lt;-seq(-3,3,0.001) fx&lt;-x^2/18 gf_line(fx~x,ylab=&quot;f(x)&quot;,title=&quot;pdf of X&quot;) %&gt;% gf_theme(theme_classic()) ggplot(data.frame(x=c(-3, 3)), aes(x)) + stat_function(fun=function(x) x^2/18) + theme_classic() + labs(y=&quot;f(x)&quot;,title=&quot;pdf of X&quot;) curve(x^2/18,from=-3,to=3,ylab=&quot;f(x)&quot;,main=&quot;pdf of X&quot;) Find and plot the cdf of \\(X\\). \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\int_{-3}^x \\frac{t^2}{18}\\mathop{}\\!\\mathrm{d}t = \\frac{t^3}{54}\\bigg|_{-3}^x = \\frac{x^3}{54}+\\frac{1}{2} \\] \\[ F_X(x)=\\left\\{\\begin{array}{ll} 0, &amp; x&lt;-3 \\\\ \\frac{x^3}{54}+\\frac{1}{2}, &amp; -3\\leq x \\leq 3 \\\\ 1, &amp; x&gt;3 \\end{array}\\right. \\] x&lt;-seq(-3.5,3.5,0.001) fx&lt;-pmin(1,(1*(x&gt;=-3)*(x^3/54+1/2))) gf_line(fx~x,ylab=&quot;F(x)&quot;,title=&quot;cdf of X&quot;) %&gt;% gf_theme(theme_classic()) Find \\(\\mbox{P}(X&lt;1)\\). \\[ \\mbox{P}(X&lt;1)=F(1)=\\frac{1}{54}+\\frac{1}{2}=0.519 \\] integrate(function(x)x^2/18,-3,1) ## 0.5185185 with absolute error &lt; 5.8e-15 Find \\(\\mbox{P}(1.5&lt;X\\leq 2.5)\\). \\[ \\mbox{P}(1.5&lt; X \\leq 2.5)=F(2.5)-F(1.5)=\\frac{2.5^3}{54}+\\frac{1}{2}-\\frac{1.5^3}{54}-\\frac{1}{2}=0.227 \\] integrate(function(x)x^2/18,1.5,2.5) ## 0.2268519 with absolute error &lt; 2.5e-15 Find the 80th percentile of \\(X\\) (the value \\(x\\) for which 80% of the distribution is to the left of that value). Need \\(x\\) such that \\(F(x)=0.8\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.8\\) for \\(x\\) yields \\(x=2.530\\). uniroot(function(x)x^3/54+.5-.8,c(-3,3)) ## $root ## [1] 2.530293 ## ## $f.root ## [1] -1.854422e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Find the value \\(x\\) such that \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\). Because this distribution is symmetric, finding \\(x\\) is equivalent to finding \\(x\\) such that \\(\\mbox{P}(X&gt;x)=0.3\\). (It helps to draw a picture). Thus, we need \\(x\\) such that \\(F(x)=0.7\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.7\\) for \\(x\\) yields \\(x=2.210\\). Find the mean and variance of \\(X\\). \\[ \\mbox{E}(X)=\\int_{-3}^3 x\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{72}\\bigg|_{-3}^3=\\frac{81}{72}-\\frac{81}{72} = 0 \\] \\[ \\mbox{E}(X^2)=\\int_{-3}^3 x^2\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^5}{90}\\bigg|_{-3}^3=\\frac{243}{90}-\\frac{-243}{90} = 5.4 \\] \\[ \\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2=5.4-0^2=5.4 \\] Simulate 10000 values from this distribution and plot the density. This is tricky since we need a cube root function. Just raising to the one-third power wont work. Lets write our own function. cuberoot &lt;- function(x) { sign(x) * abs(x)^(1/3)} set.seed(4) results &lt;- do(10000)*cuberoot((runif(1)-.5)*54) results %&gt;% gf_dens(~cuberoot) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;pdf from simulation&quot;,x=&quot;x&quot;,y=&quot;f(x)&quot;) Notice that the smoothing operation goes past the support of \\(X\\) and thus shows a concave down curve. We could clean up by limiting the x-axis to the interval [-3,3]. inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max ## ...1 cuberoot numeric -2.999981 -2.382864 -0.1574198 2.376346 2.999347 ## mean sd n missing ## ...1 -0.002416475 2.322639 10000 0 11.2.2 Problem 2 2. Let \\(X\\) be a continuous random variable. Prove that the cdf of \\(X\\), \\(F_X(x)\\) is a non-decreasing function. (Hint: show that for any \\(a &lt; b\\), \\(F_X(a) \\leq F_X(b)\\).) Let \\(a&lt;b\\), where \\(a\\) and \\(b\\) are both in the domain of \\(X\\). Note that \\(F_X(a)=\\mbox{P}(X\\leq a)\\) and \\(F_X(b)=\\mbox{P}(X\\leq b)\\). Since \\(a&lt;b\\), we can partition \\(\\mbox{P}(X\\leq b)\\) as \\(\\mbox{P}(X\\leq a)+\\mbox{P}(a &lt; X \\leq b)\\). One of the axioms of probability is that a probability must be non-negative, so I know that \\(\\mbox{P}(a &lt; X \\leq b)\\geq 0\\). Thus, \\[ \\mbox{P}(X\\leq b)=\\mbox{P}(X\\leq a)+\\mbox{P}(a &lt; X \\leq b) \\geq \\mbox{P}(X\\leq a) \\] So, we have shown that \\(F_X(a)\\leq F_X(b)\\). Thus, \\(F_X(x)\\) is a non-decreasing function. "],["DISCRETENAMED.html", "Chapter 12 Named Discrete Distributions 12.1 Objectives 12.2 Homework", " Chapter 12 Named Discrete Distributions 12.1 Objectives Recognize and setup for use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) to include parameters, assumptions, and moments. Use R to calculate probabilities and quantiles involving random variables with common discrete distributions. 12.2 Homework For each of the problems below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. We will demonstrate using 1a and 1b. 12.2.1 Problem 1 1. The T-6 training aircraft is used during UPT. Suppose that on each training sortie, aircraft return with a maintenance-related failure at a rate of 1 per 100 sorties. Find the probability of no maintenance failures in 15 sorties. \\(X\\): the number of maintenance failures in 15 sorties. \\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\) \\(\\mbox{E}(X)=15*0.01=0.15\\) and \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\). \\(\\mbox{P}(\\mbox{No maintenance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\) 0.99^15 ## [1] 0.8600584 ## or dbinom(0,15,0.01) ## [1] 0.8600584 This probability makes sense, since the expected value is fairly low. Because, on average, only 0.15 failures would occur every 15 trials, 0 failures would be a very common result. Graphically, the pmf looks like this: gf_dist(&quot;binom&quot;,size=15,prob=0.01) %&gt;% gf_theme(theme_classic()) Find the probability of at least two maintenance failures in 15 sorties. We can use the same \\(X\\) as above. Now, we are looking for \\(\\mbox{P}(X\\geq 2)\\). This is equivalent to finding \\(1-\\mbox{P}(X\\leq 1)\\): ## Directly 1-(0.99^15 + 15*0.01*0.99^14) ## [1] 0.009629773 ## or, using R sum(dbinom(2:15,15,0.01)) ## [1] 0.009629773 ## or 1-sum(dbinom(0:1,15,0.01)) ## [1] 0.009629773 ## or 1-pbinom(1,15,0.01) ## [1] 0.009629773 ## or pbinom(1,15,0.01,lower.tail = F) ## [1] 0.009629773 Find the probability of at least 30 successful (no mx failures) sorties before the first failure. \\(X\\): the number of maintenance failures out of 30 sorties. \\(X\\sim \\textsf{Binom}(n=30,p=0.01)\\), and \\(\\mbox{E}(X)=0.3\\) and \\(\\mbox{Var}(X)=0.297\\). \\(\\mbox{P}(\\mbox{0 failures})=\\mbox{P}(X=0)=0.99^{30}\\) 0.99^30 ## [1] 0.7397004 ##or dbinom(0,30,0.01) ## [1] 0.7397004 Using negative binomial, which was not in the reading but you can research: \\(Y\\): the number of successful sorties before the first failure. \\(Y\\sim \\textsf{NegBin}(n=1,p=0.01)\\), and \\(\\mbox{E}(X)=99\\) and \\(\\mbox{Var}(X)=9900\\). \\(\\mbox{P}(\\mbox{at least 30 successes before first failure})=\\mbox{P}(Y\\geq 30)\\) 1-pnbinom(29,1,0.01) ## [1] 0.7397004 Find the probability of at least 50 successful sorties before the third failure. Using a binomial random variable, we have 52 trials and need at least 50 to be a success. The random variable is \\(X\\) the number of successful sorties out of 52. 1-pbinom(49,52,.99) ## [1] 0.9846474 Or using a negative binomial, let \\(Y\\): the number of successful sorties before the third failure. \\(Y\\sim \\textsf{NegBin}(n=3,p=0.01)\\), and \\(\\mbox{E}(X)=297\\) and \\(\\mbox{Var}(X)=29700\\). \\(\\mbox{P}(\\mbox{at least 50 successes before 3rd failure})=\\mbox{P}(Y\\geq 50)\\) 1-pnbinom(49,3,0.01) ## [1] 0.9846474 Notice if the question had been exactly 50 successful sorties before the 3 failure, that is a different question. Then we could use either: dbinom(50,52,.99)*.01 ## [1] 0.000802238 The \\(0.01\\) is because the last trial is a failure. Or dnbinom(50,3,0.01) ## [1] 0.000802238 12.2.2 Problem 2 2. On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. \\(X\\): number of vehicles that arrive in 10 minutes \\(X\\sim \\textsf{Pois}(\\lambda=40/6=6.67)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=6.67\\). \\(\\mbox{P}(\\mbox{no arrivals in 10 minutes})=\\mbox{P}(X=0)=\\frac{6.67^0 e^{-6.67}}{0!}=e^{-6.67}\\) exp(-40/6) ## [1] 0.001272634 ##or dpois(0,40/6) ## [1] 0.001272634 Find the probability at least 50 vehicles arrive in an hour. \\(X\\): number of vehicles that arrive in an hour \\(X\\sim \\textsf{Pois}(\\lambda=40)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=40\\). \\(\\mbox{P}(\\mbox{at least 50 arrivals in 1 hour})=\\mbox{P}(X\\geq 50)\\) 1-ppois(49,40) ## [1] 0.07033507 Find the probability that at least 5 minutes will pass before the next arrival. \\(X\\): number of vehicles that arrive in 5 minutes \\(X\\sim \\textsf{Pois}(\\lambda=40/12=3.33)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=3.33\\). \\(\\mbox{P}(\\mbox{no arrivals in 5 minutes})=\\mbox{P}(X=0)=\\frac{3.33^0 e^{-3.33}}{0!}=e^{-3.33}\\) exp(-40/12) ## [1] 0.03567399 ##or dpois(0,40/12) ## [1] 0.03567399 12.2.3 Problem 3 3. Suppose there are 12 male and 7 female cadets in a classroom. I select 5 completely at random (without replacement). Find the probability I select no female cadets. \\(X\\): number of female cadets selected out of sample of size 5 \\(X\\sim \\textsf{Hypergeom}(m=7,n=12,k=5)\\) and \\(\\mbox{E}(X)=1.842\\) and \\(\\mbox{Var}(X)=0.905\\). \\[ \\mbox{P}(\\mbox{no female cadets selected})=\\mbox{P}(X=0)=\\frac{{7\\choose 0}{12\\choose 5}}{{19\\choose 5}} \\] choose(12,5)/choose(19,5) ## [1] 0.06811146 ##or dhyper(0,7,12,5) ## [1] 0.06811146 Find the probability I select more than 2 female cadets. Using the same random variable: \\[ \\mbox{P}(\\mbox{more than 2 female})=\\mbox{P}(X&gt;2)=1-\\mbox{P}(X\\leq 2) \\] 1-phyper(2,7,12,5) ## [1] 0.2365841 ##or sum(dhyper(3:5,7,12,5)) ## [1] 0.2365841 "],["CONTNNAMED.html", "Chapter 13 Named Continuous Distributions 13.1 Objectives 13.2 Homework", " Chapter 13 Named Continuous Distributions 13.1 Objectives Recognize when to use common continuous distributions (Uniform, Exponential, Gamma, Normal, Beta), identify parameters, and find moments. Use R to calculate probabilities and quantiles involving random variables with common continuous distributions. Understand the relationship between the Poisson process and the Poisson &amp; Exponential distributions. Know when to apply and then use the memoryless property. 13.2 Homework For problems 1-3 below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. 13.2.1 Problem 1 1. On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. \\(X\\): number of vehicles that arrive in 10 minutes \\(X\\sim \\textsf{Pois}(\\lambda=40/6=6.67)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=6.67\\). \\(\\mbox{P}(\\mbox{no arrivals in 10 minutes})=\\mbox{P}(X=0)=\\frac{6.67^0 e^{-6.67}}{0!}=e^{-6.67}\\) exp(-40/6) ## [1] 0.001272634 ##or dpois(0,40/6) ## [1] 0.001272634 or, using the exponential distribution: \\(Y\\): time in minutes until the next arrival \\(Y\\sim \\textsf{Expon}(\\lambda=40/60=0.667)\\) and \\(\\mbox{E}(Y)=1.5\\) and \\(\\mbox{Var}(Y)=2.25\\). \\[ \\mbox{P}(\\mbox{at least 10 minutes until the next arrival})=\\mbox{P}(Y\\geq 10)=\\int_{10}^\\infty \\frac{2}{3}e^{-\\frac{2}{3}y}\\,\\mathrm{d}y \\] 1-pexp(10,2/3) ## [1] 0.001272634 or using simulation: set.seed(616) mean(rpois(100000,40/6) == 0) ## [1] 0.00126 mean(rexp(100000,2/3) &gt;=10) ## [1] 0.00127 Find the probability that at least 5 minutes will pass before the next arrival. \\(Y\\): same as in part a \\[ \\mbox{P}(\\mbox{at least 5 minutes until next arrival})=\\mbox{P}(Y\\geq 5)=\\int_{5}^\\infty \\frac{2}{3}e^{-\\frac{2}{3}y}\\,\\mathrm{d}y \\] 1-pexp(5,2/3) ## [1] 0.03567399 Find the probability that the next vehicle will arrive between 2 and 10 minutes from now. Same \\(Y\\) as defined above. pexp(10,2/3)-pexp(2,2/3) ## [1] 0.2623245 Find the probability that at least 7 minutes will pass before the next arrival, given that 2 minutes have already passed. Compare this answer to part (b). This is an example of the memoryless property of the exponential distribution. \\[ \\mbox{P}(Y\\geq 7|Y\\geq 2) = \\frac{\\mbox{P}(Y\\geq 7, Y\\geq 2)}{\\mbox{P}(Y\\geq 2)} = \\frac{\\mbox{P}(Y\\geq 7)}{\\mbox{P}(Y\\geq 2)} \\] (1-pexp(7,2/3))/(1-pexp(2,2/3)) ## [1] 0.03567399 This is the same answer and a result of the memoryless property. Fill in the blank. There is a probability of 90% that the next vehicle will arrive within __ minutes. This value is known as the 90% percentile of the random variable. qexp(0.9,2/3) ## [1] 3.453878 Use the function stripplot() to visualize the arrival of 30 vehicles using a random sample from the appropriate exponential distribution. set.seed(202) stripplot(cumsum(rexp(30,2/3)),xlab=&quot;Arrival Time&quot;) 13.2.2 Problem 2 2. Suppose time until computer errors on the F-35 follows a Gamma distribution with mean 20 hours and variance 10. Find the probability that 20 hours pass without a computer error. \\(X\\): time in hours until next computer error. \\(X\\sim \\textsf{Gamma}(\\alpha = 40, \\lambda = 2)\\) We need to find \\(\\alpha\\) and \\(\\lambda\\) from the given moments. \\(\\mbox{E}(X) = 20 = \\frac{\\alpha}{\\lambda}\\) \\(\\mbox{Var}(X) = 10 = \\frac{\\alpha}{\\lambda^2}\\) Notice that \\(\\frac{\\mbox{E}(X)}{\\mbox{Var}(X)} = \\lambda = \\frac{20}{10}=2\\) and then using \\(\\mbox{E}(X) = 20 = \\frac{\\alpha}{\\lambda}\\) we get \\(\\alpha = 40\\). \\(\\mbox{P}(X\\geq 20)\\): 1-pgamma(20,shape=40,rate=2) ## [1] 0.4789711 Find the probability that 45 hours pass without a computer error, given that 25 hours have already passed. Does the memoryless property apply to the Gamma distribution? \\[ P(X\\geq 45|X\\geq 25) = \\frac{P(X\\geq 45, X\\geq 25)}{P(X\\geq 25)} = \\frac{P(X\\geq 45)}{P(X\\geq 25)} \\] (1-pgamma(45,40,2))/(1-pgamma(25,40,2)) ## [1] 1.77803e-08 No, the memoryless property does not apply to the Gamma distribution. Find \\(a\\) and \\(b\\) where there is a 95% probability that the time until next computer error will be between \\(a\\) and \\(b\\). (Note: technically, there are many answers to this question, but find \\(a\\) and \\(b\\) such that each tail has equal probability.) qgamma(c(0.025,0.975),40,2) ## [1] 14.28829 26.65714 So in the time interval \\([14,29,26.66]\\). qgamma(.95,40,2) ## [1] 25.46987 Another answer is between \\([0,25,47]\\). 13.2.3 Problem 3 3. Suppose PFT scores in the cadet wing follow a normal distribution with mean 330 and standard deviation 50. Find the probability a randomly selected cadet has a PFT score higher than 450. \\(X\\): PFT score of a randomly selected cadet \\(X\\sim \\textsf{Norm}(\\mu=330,\\sigma=50)\\) \\(\\mbox{E}(X) = 330\\) and \\(\\mbox{Var}(X)=50^2=2500\\). 1-pnorm(450,330,50) ## [1] 0.008197536 Find the probability a randomly selected cadet has a PFT score within 2 standard deviations of the mean. Need \\(\\mbox{P}(230 \\leq X \\leq 430)\\). pnorm(430,330,50)-pnorm(230,330,50) ## [1] 0.9544997 Find \\(a\\) and \\(b\\) such that 90% of PFT scores will be between \\(a\\) and \\(b\\). Need \\(a\\) such that \\(\\mbox{P}(X\\leq a)=0.05\\) and \\(b\\) such that \\(\\mbox{P}(X\\geq b)=0.05\\): qnorm(0.05,330,50) ## [1] 247.7573 qnorm(0.95,330,50) ## [1] 412.2427 Find the probability a randomly selected cadet has a PFT score higher than 450 given he/she is among the top 10% of cadets. Need \\(\\mbox{P}(X&gt;450|X&gt;x_{0.9})\\) where \\(x_{0.9}\\) is the 90th percentile of \\(X\\). The 90th percentile is: qnorm(0.9,330,50) ## [1] 394.0776 \\[ \\mbox{P}(X&gt;450|X&gt;x_{0.9})=\\frac{\\mbox{P}(X&gt;450, X&gt;x_{0.9})}{\\mbox{P}(X&gt;x_{0.9})}=\\frac{\\mbox{P}(X&gt;450, X&gt;394.08)}{\\mbox{P}(X&gt;x_{0.9})}=\\frac{\\mbox{P}(X&gt;450)}{0.1} \\] This is assuming that \\(x_{0.9}&lt;450\\). Otherwise the problem is trivial and the probability is 1. (1-pnorm(450,330,50))/0.1 ## [1] 0.08197536 13.2.4 Problem 4 4. Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show that \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write out the beta distribution pdf where \\(\\alpha=1\\) and \\(\\beta=1\\). The beta pdf is: \\[ f_X(x)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\] When \\(X\\sim\\textsf{Beta}(\\alpha=1,\\beta=1)\\), this becomes: \\[ f_X(x)=\\frac{\\Gamma(2)}{\\Gamma(1)\\Gamma(1)}x^{1-1}(1-x)^{1-1} = 1 \\] 13.2.5 Problem 5 5. When using R to calculate probabilities related to the gamma distribution, we often use pgamma. Recall that pgamma is equivalent to the cdf of the gamma distribution. If \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), then \\[ \\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)} \\] The dgamma function exists in R too. In plain language, explain what dgamma returns. Im not looking for the definition found in R documentation. Im looking for a simple description of what that function returns. Is the output of dgamma useful? If so, how? The dgamma function returns the value of probability density function. While this is not a probability, it is still a useful quantity. It can be said that larger densities (\\(f(x)\\)) imply that values near \\(x\\) are more likely to occur than values associated with smaller densities. It is also useful when computing conditional probability distributions. 13.2.6 Problem 6 6. Advanced. You may have heard of the 68-95-99.7 rule. This is a helpful rule of thumb that says if a population has a normal distribution, then 68% of the data will be within one standard deviation of the mean, 95% of the data will be within two standard deviations and 99.7% of the data will be within three standard deviations. Create a function in R that has two inputs (a mean and a standard deviation). It should return a vector with three elements: the probability that a randomly selected observation from the normal distribution with the inputted mean and standard deviation lies within one, two and three standard deviations. Test this function with several values of mu and sd. You should get the same answer each time. rulethumb&lt;-function(mu,sd){ pnorm(mu+c(1,2,3)*sd,mu,sd)-pnorm(mu-c(1,2,3)*sd,mu,sd) } rulethumb(15,12) ## [1] 0.6826895 0.9544997 0.9973002 rulethumb(0,1) ## [1] 0.6826895 0.9544997 0.9973002 13.2.7 Problem 7 7. Derive the mean of a general uniform distribution, \\(U(a,b)\\). From the definition \\[E(X)=\\int_{a}^{b}xf(x)dx=\\] \\[ =\\int_{a}^{b}\\frac{x}{b-a}dx =\\] \\[ =\\frac{1}{b-a}\\int_{a}^{b}xdx = \\frac{1}{b-a}\\cdot\\frac{x^2}{2}\\bigg|_{a}^{b}=\\] \\[ =\\frac{1}{b-a}\\cdot\\frac{b^2-a^2}{2}= \\frac{1}{b-a}\\cdot\\frac{(b-a)(b+a)}{2}=\\frac{(b+a)}{2}\\] "],["MULTIDISTS.html", "Chapter 14 Multivariate Distributions 14.1 Objectives 14.2 Homework", " Chapter 14 Multivariate Distributions 14.1 Objectives Define (and distinguish between) the terms joint probability mass/density function, marginal pmf/pdf, and conditional pmf/pdf. Given a joint pmf/pdf, obtain the marginal and conditional pmfs/pdfs. Use joint, marginal and conditional pmfs/pdfs to obtain probabilities. 14.2 Homework 14.2.1 Problem 1 1. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=x + y \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). Verify that \\(f\\) is a valid pdf. \\[ \\int_0^1\\int_0^1 x+y\\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^1 xy + \\frac{y^2}{2}\\bigg|_0^1 \\,\\mathrm{d}x = \\int_0^1 x+\\frac{1}{2}\\,\\mathrm{d}x = \\frac{x^2}{2}+\\frac{x}{2}\\bigg|_0^1=1 \\] Or library(cubature) # load the package &quot;cubature&quot; f &lt;- function(x) { (x[1] + x[2]) } # &quot;x&quot; is vector adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1)) ## $integral ## [1] 1 ## ## $error ## [1] 0 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 Find the marginal pdfs of \\(X\\) and \\(Y\\). \\[ f_X(x)=\\int_0^1 x+y\\,\\mathrm{d}y = xy + \\frac{y^2}{2}\\bigg|_0^1 = x+\\frac{1}{2} \\] where \\(0\\leq x \\leq 1\\). Similarly, \\(f_Y(y)=y+\\frac{1}{2}\\) for \\(0 \\leq y \\leq 1\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[ f_{X|Y=y}(x)=\\frac{x+y}{y+\\frac{1}{2}} \\] where \\(0\\leq x \\leq 1\\). Similarly, \\(f_{Y|X=x}(y)=\\frac{x+y}{x+\\frac{1}{2}}\\) for \\(0\\leq y \\leq 1\\). Find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;0.8)\\); \\(\\mbox{P}(X&lt;0.2,Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y= 0.25)\\); Optional - \\(\\mbox{P}(X\\leq Y)\\). \\[ \\mbox{P}(X&lt;0.5)=\\int_0^{0.5} x+\\frac{1}{2}\\,\\mathrm{d}x = \\frac{x^2}{2}+\\frac{x}{2}\\bigg|_0^{0.5}=0.375 \\] integrate(function(x)(x+1/2),0,1/2) ## 0.375 with absolute error &lt; 4.2e-15 Or using multivariate integration, integrate out \\(y\\). adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1/2, 1)) ## $integral ## [1] 0.375 ## ## $error ## [1] 5.551115e-17 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{P}(Y&lt;0.8)=\\int_{0.8}^1 y+\\frac{1}{2}\\,\\mathrm{d}y = \\frac{y^2}{2}+\\frac{y}{2}\\bigg|_{0.8}^1=1-0.72=0.28 \\] adaptIntegrate(f, lowerLimit = c(0, 0.8), upperLimit = c(1, 1)) ## $integral ## [1] 0.28 ## ## $error ## [1] 5.551115e-17 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{P}(X&lt;0.2,Y\\geq 0.75)=\\int_0^{0.2}\\int_{0.75}^1 x+y\\,\\mathrm{d}y \\,\\mathrm{d}x= \\int_0^{0.2} xy+\\frac{y^2}{2}\\bigg|_{0.75}^1\\,\\mathrm{d}x \\] \\[ =\\int_0^{0.2} x+\\frac{1}{2}-\\frac{3x}{4}-\\frac{9}{32}\\,\\mathrm{d}x = \\int_0^{0.2} \\frac{x}{4}+\\frac{7}{32}\\,\\mathrm{d}x = \\frac{x^2}{8}+\\frac{7x}{32}\\bigg|_0^{0.2}=0.04875 \\] adaptIntegrate(f, lowerLimit = c(0, 0.75), upperLimit = c(0.2, 1)) ## $integral ## [1] 0.04875 ## ## $error ## [1] 0 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{P}(X&lt;0.2|Y\\geq 0.75)=\\frac{\\mbox{P}(X&lt;0.2,Y\\geq 0.75)}{\\mbox{P}(Y\\geq 0.75)}=\\frac{0.04875}{\\int_{0.75}^1 y+\\frac{1}{2}\\,\\mathrm{d}y}=\\frac{0.04875}{0.34375} \\approx 0.142 \\] For \\[ \\mbox{P}(X&lt;0.2|Y= 0.25) \\] we need \\[ f_{X|Y=.25}(x)=\\frac{x+y}{y+\\frac{1}{2}}\\bigg|_{y=0.25}=\\frac{x+.25}{.25+\\frac{1}{2}}=\\frac{x+.25}{.75}=\\frac{4x+1}{3} \\] \\[ \\mbox{P}(X&lt;0.2|Y= 0.25) = \\int_{0}^{0.2} \\frac{4x+1}{3} \\,\\mathrm{d}x \\] \\[ =\\frac{1}{3}\\left( 2x^2 +x \\right) \\bigg|_0^{0.2} = \\frac{1}{3}\\left( 2\\cdot0.2^2 +0.2 \\right) \\approx 0.0933 \\] f2 &lt;- function(x) { (4*x[1] + 1)/3 } # &quot;x&quot; is vector adaptIntegrate(f2, lowerLimit = c(0), upperLimit = c(0.2)) ## $integral ## [1] 0.09333333 ## ## $error ## [1] 1.036208e-15 ## ## $functionEvaluations ## [1] 15 ## ## $returnCode ## [1] 0 Optional \\[ \\mbox{P}(X\\leq Y)=\\int_0^1\\int_0^y x+y \\,\\mathrm{d}x \\,\\mathrm{d}y = \\int_0^1 xy+\\frac{x^2}{2}\\bigg|_0^y \\,\\mathrm{d}x = \\int_0^1 \\frac{3y^2}{2}\\,\\mathrm{d}y = \\frac{y^3}{2}\\bigg|_0^1 = \\frac{1}{2} \\] 14.2.2 Problem 2 2. In the Notes, we saw an example where \\(f_X(x)=f_{X|Y=y}(x)\\) and \\(f_Y(y)=f_{Y|X=x}(y)\\). This is not common and is important. What does this imply about \\(X\\) and \\(Y\\)? Since the conditional density function is always equal to the marginal, it means that \\(X\\) and \\(Y\\) are independent of one another. Also, if the conditioning variable does not appear in the conditional density function and the domain of the joint density is rectangular, the bounds of the two variables are constants, the random variables are independent. The variables in the previous problem are dependent, look at the conditional density functions to see that the conditional density depends on the conditioned variable. 14.2.3 Problem 3 3. ADVANCED: Recall on an earlier assignment, we came up with random variables to describe timeliness at an airport. Suppose over the course of 210 days, on each day we recorded the number of customer complaints regarding timeliness. Also on each day, we recorded the weather (our airport is located somewhere without snow and without substantial wind). The data are displayed below. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Weather Status} &amp; \\\\ &amp; &amp; \\mbox{Clear} &amp; \\mbox{Light Rain} &amp; \\mbox{Rain} \\\\ &amp;\\hline 0 &amp; 28 &amp; 11 &amp; 4 \\\\ \\textbf{num complaints} &amp; 1 &amp; 18 &amp; 15 &amp; 8 \\\\ &amp; 2 &amp; 17 &amp; 25 &amp; 12 \\\\ &amp; 3 &amp; 13 &amp; 15 &amp; 16 \\\\ &amp; 4 &amp; 8 &amp; 8 &amp; 10 \\\\ &amp; 5 &amp; 0 &amp; 1 &amp; 1 \\\\ \\end{array} \\] First, define two random variables for this scenario. One of them (# of complaints) is essentially already a random variable. For the other (weather status) you will need to assign a number to each status. Use the table above to build an empirical joint pmf of the two random variables. We will simply label the weather random variable as 0, 1, 2. We convert to probabilities by dividing by 210. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Weather Status} &amp; \\\\ &amp; &amp; \\mbox{Clear} &amp; \\mbox{Light Rain} &amp; \\mbox{Rain} \\\\ &amp;\\hline 0 &amp; 0.133 &amp; 0.052 &amp; 0.019 \\\\ \\textbf{num complaints} &amp; 1 &amp; 0.086 &amp; 0.071 &amp; 0.038 \\\\ &amp; 2 &amp; 0.081 &amp; 0.119 &amp; 0.057 \\\\ &amp; 3 &amp; 0.062 &amp; 0.071 &amp; 0.076 \\\\ &amp; 4 &amp; 0.038 &amp; 0.038 &amp; 0.048 \\\\ &amp; 5 &amp; 0 &amp; 0.005 &amp; 0.005 \\\\ \\end{array} \\] Find the marginal pmfs of each random variable. \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0.400, &amp; x=0 \\\\ 0.357, &amp; x=1 \\\\ 0.243, &amp; x=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] \\[ f_Y(y)=\\left\\{\\begin{array}{ll} 0.205, &amp; y=0 \\\\ 0.195, &amp; y=1 \\\\ 0.257, &amp; y=2 \\\\ 0.210, &amp; y=3 \\\\ 0.124, &amp; y=4 \\\\ 0.010, &amp; y=5 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Find the probability of fewer than 3 complaints. \\[ \\mbox{P}(Y&lt;3)=0.205+0.195+0.257=0.657 \\] Find the probability of fewer than 3 complaints given there is no rain. \\[ \\mbox{P}(Y&lt;3|X=0)=\\frac{0.133+0.086+0.081}{0.4}=0.75 \\] 14.2.4 Problem 4 Optional for those of you that like Calc III and want a challenge. 4. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=1 \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 2x\\). Verify that \\(f\\) is a valid pdf. \\[ \\int_0^1 \\int_0^{2x} 1 \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^1 y\\bigg|_0^{2x}\\,\\mathrm{d}x = \\int_0^1 2x\\,\\mathrm{d}x = x^2\\bigg|_0^1 = 1 \\] Find the marginal pdfs of \\(X\\) and \\(Y\\). \\[ f_X(x)=\\int_0^{2x} 1 \\,\\mathrm{d}y = y\\bigg|_0^{2x}=2x \\] where \\(0\\leq x \\leq 1\\). \\[ f_Y(y)=\\int_{y/2}^1 1 \\,\\mathrm{d}x = x\\bigg|_{y/2}^1 = 1-\\frac{y}{2} \\] where \\(0 \\leq y \\leq 2\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[ f_{X|Y=y}(x)=\\frac{1}{1-\\frac{y}{2}}=\\frac{2}{2-y} \\] where \\(y/2 \\leq x \\leq 1\\). \\[ f_{Y|X=x}(y)=\\frac{1}{2x} \\] where \\(0\\leq y \\leq 2x\\). Find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;1)\\); \\(\\mbox{P}(X&lt;0.5,Y\\leq 0.8)\\); \\(\\mbox{P}(X&lt;0.5|Y= 0.8)\\); Optional \\(\\mbox{P}(Y\\leq 1-X)\\). (It would probably help to draw some pictures.) \\[ \\mbox{P}(X&lt;0.5)=\\int_0^{0.5} 2x \\,\\mathrm{d}x = x^2\\bigg|_0^{0.5}=0.25 \\] \\[ \\mbox{P}(Y&gt;1)=\\int_1^2 1-\\frac{y}{2}\\,\\mathrm{d}y = y-\\frac{y^2}{4}\\bigg|_1^2 = 1-\\frac{3}{4}=0.25 \\] \\[ \\mbox{P}(X&lt;0.5,Y\\leq 0.8)=\\int_0^{0.4}\\int_0^{2x} 1 \\,\\mathrm{d}y \\,\\mathrm{d}x + \\int_{0.4}^{0.5}\\int_0^{0.8} 1 \\,\\mathrm{d}y \\,\\mathrm{d}x = 0.16+0.08=0.24 \\] \\[ \\mbox{P}(X&lt;0.5|Y= 0.8)=\\int_{0.4}^{0.5} \\frac{2}{2-0.8}\\,\\mathrm{d}x = \\frac{5x}{3}\\bigg|_{0.4}^{0.5}=0.1667 \\] \\[ \\mbox{P}(Y\\leq 1-X)=\\int_0^{1/3}\\int_0^{2x} 1 \\,\\mathrm{d}y \\,\\mathrm{d}x + \\int_{1/3}^1\\int_0^{1-x}1 \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^{1/3}2x\\,\\mathrm{d}x + \\int_{1/3}^1 1-x\\,\\mathrm{d}x \\] \\[ =\\frac{1}{9}+\\left(x-\\frac{x^2}{2}\\right)_{1/3}^1=\\frac{1}{9}+\\frac{1}{2}-\\frac{1}{3}+\\frac{1}{18} = \\frac{1}{3} \\] "],["MULTIEXP.html", "Chapter 15 Multivariate Expectation 15.1 Objectives 15.2 Homework", " Chapter 15 Multivariate Expectation 15.1 Objectives Given a joint pmf/pdf, obtain means and variances of random variables and functions of random variables. Define the terms covariance and correlation, and given a joint pmf/pdf, obtain the covariance and correlation between two random variables. Given a joint pmf/pdf, determine whether random variables are independent of one another. Find conditional expectations. 15.2 Homework 15.2.1 Problem 1 1. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[ f_{X,Y}(x,y)=x + y \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). Find \\(\\mbox{E}(X)\\) and \\(\\mbox{E}(Y)\\). We will use the marginal pdfs found in the Application 14 solution. \\[ \\mbox{E}(X)=\\int_0^1 x\\left(x+\\frac{1}{2}\\right)\\,\\mathrm{d}x=\\frac{x^3}{3}+\\frac{x^2}{4}\\bigg|_0^1=\\frac{1}{3}+\\frac{1}{4}=\\frac{7}{12}=0.583 \\] Or numerically: f &lt;- function(x) { x[1]*(x[1] + x[2]) } # &quot;x&quot; is vector adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1)) ## $integral ## [1] 0.5833333 ## ## $error ## [1] 1.110223e-16 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{E}(Y)=\\int_0^1 y\\left(y+\\frac{1}{2}\\right)\\,\\mathrm{d}y = 0.583 \\] Find \\(\\mbox{Var}(X)\\) and \\(\\mbox{Var}(Y)\\). \\[ \\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2 \\] \\[ \\mbox{E}(X^2)=\\int_0^1 x^2\\left(x+\\frac{1}{2}\\right)\\,\\mathrm{d}x = \\frac{x^4}{4}+\\frac{x^3}{6}\\bigg|_0^1=\\frac{1}{ 4}+\\frac{1}{6}=\\frac{5}{12}=0.417 \\] As a check: f &lt;- function(x) { x[1]^2*(x[1] + x[2]) } # &quot;x&quot; is vector round(adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1))$integral,3) ## [1] 0.417 So, \\(\\mbox{Var}(X)=0.417-0.583^2=0.076\\). Similarly, \\(\\mbox{Var}(Y)=0.076\\). Find \\(\\mbox{Cov}(X,Y)\\) and \\(\\rho\\). Are \\(X\\) and \\(Y\\) independent? \\[ \\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y) \\] \\[ \\mbox{E}(XY)=\\int_0^1\\int_0^1 xy(x+y)\\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^1 \\frac{x^2y^2}{2}+\\frac{xy^3}{3}\\bigg|_0^1 \\,\\mathrm{d}x = \\int_0^1 \\frac{x^2}{2}+\\frac{x}{3}\\,\\mathrm{d}x \\] \\[ =\\frac{x^3}{6}+\\frac{x^2}{6}\\bigg|_0^1=\\frac{1}{ 3}=0.333 \\] As a check: f &lt;- function(x) { x[1]*x[2]*(x[1] + x[2]) } # &quot;x&quot; is vector round(adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1))$integral,3) ## [1] 0.333 So, \\[ \\mbox{Cov}(X,Y)=\\frac{1}{3}-\\left(\\frac{7}{12}\\right)^2=-0.007 \\] \\[ \\rho=\\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}=\\frac{-0.007}{\\sqrt{0.076\\times0.076}}=-0.0909 \\] As a check: -0.007/sqrt(.076^2) ## [1] -0.09210526 Using exact values: (1/3-(7/12)^2)/sqrt((5/12-(7/12)^2)^2) ## [1] -0.09090909 With a non-zero covariance, \\(X\\) and \\(Y\\) are not independent. Find \\(\\mbox{Var}(3X+2Y)\\). \\[ \\mbox{Var}(3X+2Y)=\\mbox{Var}(3X)+\\mbox{Var}(2Y)+2\\mbox{Cov}(3X,2Y)= \\] \\[ 9\\mbox{Var}(X)+4\\mbox{Var}(Y)+12\\mbox{Cov}(X,Y) = \\] \\[ 9*0.076+4*0.076+12*-0.007 = 0.910 \\] 15.2.2 Problem 2 2. Optional - not difficult but does have small Calc III idea. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=1 \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 2x\\). Find \\(\\mbox{E}(X)\\) and \\(\\mbox{E}(Y)\\). \\[ \\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\,\\mathrm{d}x = \\frac{2x^3}{3}\\bigg|_0^1=0.667 \\] \\[ \\mbox{E}(Y)=\\int_0^2 y\\left(1-\\frac{y}{2}\\right)\\,\\mathrm{d}y = \\frac{y^2}{2}-\\frac{y^3}{6}\\bigg|_0^2=2-\\frac{8}{ 6}=0.667 \\] Find \\(\\mbox{Var}(X)\\) and \\(\\mbox{Var}(Y)\\). \\[ \\mbox{E}(X^2)=\\int_0^1 x^2\\cdot 2x\\,\\mathrm{d}x = \\frac{x^4}{2}\\bigg|_0^1=0.5 \\] So, \\(\\mbox{Var}(X)=0.5-\\left(\\frac{2}{3}\\right)^2=\\frac{1}{ 18}=0.056\\) \\[ \\mbox{E}(Y^2)=\\int_0^2 y^2\\left(1-\\frac{y}{2}\\right)\\,\\mathrm{d}y = \\frac{y^3}{3}-\\frac{y^4}{8}\\bigg|_0^2=\\frac{8}{ 3}-2=0.667 \\] So, \\(\\mbox{Var}(Y)=\\frac{2}{ 3}-\\left(\\frac{2}{3}\\right)^2=\\frac{2}{9}=0.222\\) Find \\(\\mbox{Cov}(X,Y)\\) and \\(\\rho\\). Are \\(X\\) and \\(Y\\) independent? \\[ \\mbox{E}(XY)=\\int_0^1\\int_0^{2x} xy\\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^1 \\frac{xy^2}{2}\\bigg|_0^{2x}\\,\\mathrm{d}x = \\int_0^1 2x^3\\,\\mathrm{d}x = \\frac{x^4}{2}\\bigg|_0^1=\\frac{1}{2} \\] So, \\[ \\mbox{Cov}(X,Y)=\\frac{1}{2}-\\frac{2}{3}\\frac{2}{3}=\\frac{1}{18}=0.056 \\] \\[ \\rho=\\frac{\\mbox{Cov}(X,Y)}{ \\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}=\\frac{\\frac{1}{ 18}}{\\sqrt{\\frac{1}{18}\\frac{2}{9}}}=0.5 \\] \\(X\\) and \\(Y\\) appear to be positively correlated (thus not independent). Find \\(\\mbox{Var}\\left(\\frac{X}{2}+2Y\\right)\\). \\[ \\mbox{Var}\\left(\\frac{X}{2}+2Y\\right) = \\frac{1}{ 4}\\mbox{Var}(X)+4\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)=\\frac{1}{72}+\\frac{8}{ 9}+\\frac{1}{9}=1.014 \\] 15.2.3 Problem 3 3. Suppose \\(X\\) and \\(Y\\) are independent random variables. Show that \\(\\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y)\\). If \\(X\\) and \\(Y\\) are independent, then \\(\\mbox{Cov}(X,Y)=0\\). So, \\[ \\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)=0 \\] Thus, \\[ \\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y) \\] 15.2.4 Problem 4 4. You are playing a game with a friend. Each of you roll a fair sided die and record the result. Write the joint probability mass function. Let \\(X\\) be the number on your die and \\(Y\\) be the number on your friends die. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{X} &amp; \\\\ &amp; &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\ &amp;\\hline 1 &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} \\\\ &amp; 2 &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} \\\\ \\textbf{Y}&amp; 3 &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} \\\\ &amp; 4 &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} \\\\ &amp; 5 &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} \\\\ &amp; 6 &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} &amp; \\frac{1}{36} \\\\ \\end{array} \\] Find the expected value of the product of your score and your friends score. To find \\(E[XY]\\), we determine all 36 values of the product of \\(X\\) and \\(Y\\) and multiply by the associated probabilities. Since the probabilities are all equal, we will take the \\(\\frac{1}{36}\\) out of the summation. Now \\[E[XY]=\\frac{1}{36}(1+2+3+4+5+6+2+4+\\] \\[6+8+10+12+3+6+9+12+15+18+4+8+12+16+20+24+\\] \\[5+10+15+20+25+30+6+12+18+24+30+36)\\] \\[=12.25\\] Verify the previous part using simulation. set.seed(1012) (do(100000)*(sample(1:6,size=2,replace=TRUE))) %&gt;% mutate(prod=V1*V2) %&gt;% summarize(Expec=mean(prod)) ## Expec ## 1 12.25016 Using simulation, find the expected value of the maximum number on the two roles. (do(100000)*max(sample(1:6,size=2,replace=TRUE))) %&gt;% summarize(Expec=mean(max)) ## Expec ## 1 4.4737 15.2.5 Problem 5 5. A miner is trapped in a mine containing three doors. The first door leads to a tunnel that takes him to safety after two hours of travel. The second door leads to a tunnel that returns him to the mine after three hours of travel. The third door leads to a tunnel that returns him to his mine after five hours. Assuming that the miner is at all times equally likely to choose any one of the doors, yes a bad assumption but it makes for a nice problem, what is the expected length of time until the miner reaches safety? Simulating this is a little more challenging because we need a conditional but we try it first before going to the mathematical solution. Lets write a function that takes a vector and returns the sum of the values up to the first time the number 2 appears, we are using the time values as our sample space. Anytime you are repeating something more than 5 times, it might make sense to write a function. miner_time &lt;- function(x){ index &lt;- which(x==2)[1] total&lt;-cumsum(x) return(total[index]) } set.seed(113) (do(10000)*miner_time(sample(c(2,3,5),size=20,replace=TRUE))) %&gt;% summarise(Exp=mean(miner_time)) ## Exp ## 1 10.0092 Now lets find it mathematically. Let \\(X\\) be the time it takes and \\(Y\\) the door. Then we have \\[E[X] = E[E[X|Y]] \\] \\[ = \\frac{1}{3}E[X|Y=1]+\\frac{1}{3}E[X|Y=2]+\\frac{1}{3}E[X|Y=3]\\] Now if door 2 is selected \\[E[X|Y=2]=E[X]+3\\] since the miner will travel for 3 hours and then be back at the starting point. Likewise if door 3 is select \\[E[X|Y=2]=E[X]+5\\] So \\[ E[x]= \\frac{1}{3}2+\\frac{1}{3}\\left( E[X]+3 \\right)+\\frac{1}{3}\\left( E[X]+5 \\right)\\] \\[E[x] - \\frac{2}{3}E[X] = \\frac{2}{3}+\\frac{3}{3}+\\frac{5}{3}\\] \\[\\frac{1}{3}E[X]=\\frac{10}{3}\\] \\[E[X]=10\\] 15.2.6 Problem 6 6. ADVANCED: Let \\(X_1,X_2,...,X_n\\) be independent, identically distributed random variables. (This is often abbreviated as iid). Each \\(X_i\\) has mean \\(\\mu\\) and variance \\(\\sigma^2\\) (i.e., for all \\(i\\), \\(\\mbox{E}(X_i)=\\mu\\) and \\(\\mbox{Var}(X_i)=\\sigma^2\\)). Let \\(S=X_1+X_2+...+X_n=\\sum_{i=1}^n X_i\\). And let \\(\\bar{X}={\\sum_{i=1}^n \\frac{X_i}{n}}\\). Find \\(\\mbox{E}(S)\\), \\(\\mbox{Var}(S)\\), \\(\\mbox{E}(\\bar{X})\\) and \\(\\mbox{Var}(\\bar{X})\\). \\[ \\mbox{E}(S)=\\mbox{E}(X_1+X_2+...+X_n)=\\mbox{E}(X_1)+\\mbox{E}(X_2)+...+\\mbox{E}(X_n)=\\mu+\\mu+...+\\mu=n\\mu \\] Since the \\(X_i\\)s are all independent: \\[ \\mbox{Var}(S)=\\mbox{Var}(X_1+X_2+...+X_n)=\\mbox{Var}(X_1)+\\mbox{Var}(X_2)+...+\\mbox{Var}(X_n)=n\\sigma^2 \\] \\[ \\mbox{E}(\\bar{X})=\\frac{1}{n}\\mbox{E}(X_1+X_2+...+X_n)=\\frac{1}{n}n\\mu=\\mu \\] \\[ \\mbox{Var}(\\bar{X})=\\frac{1}{n^2}\\mbox{Var}(X_1+X_2+...+X_n)=\\frac{1}{n^2}n\\sigma^2=\\frac{\\sigma^2}{n} \\] "],["TRANS.html", "Chapter 16 Transformations 16.1 Objectives 16.2 Homework", " Chapter 16 Transformations 16.1 Objectives Given a discrete random variable, determine the distribution of a transformation of that random variable. Given a continuous random variable, use the cdf method to determine the distribution of a transformation of that random variable. Use simulation methods to find the distribution of a transform of single or multivariate random variables. 16.2 Homework 16.2.1 Problem 1 1. Let \\(X\\) be a random variable and let \\(g\\) be a function. By this point, it should be clear that \\(\\mbox{E}[g(X)]\\) is not necessarily equal to \\(g(\\mbox{E}[X])\\). Let \\(X\\sim \\textsf{Expon}(\\lambda=0.5)\\) and \\(g(X)=X^2\\). We know that \\(\\mbox{E}(X)=\\frac{1}{0.5}=2\\) so \\(g(\\mbox{E}(X))=\\mbox{E}(X)^2=4\\). Use R to find \\(\\mbox{E}[g(X)]\\). Make use of the fact that R has rexp() built into it, so you dont have to create your own random variable generator. Let \\(Y=X^2\\). sims&lt;-rexp(10000,0.5) mean(sims^2) ## [1] 8.031624 So, \\(g(\\mbox{E}(X))=4\\) and \\(\\mbox{E}(g(X))\\approx 7.84\\). 16.2.2 Problem 2 2. Let \\(X\\sim \\textsf{Binom}(n,\\pi)\\). What is the pmf for \\(X+3\\)? Make sure you specify the domain of \\(Y\\). [Note, we have used \\(p\\) for the probability of success in a binomial distribution in past lessons but some references use \\(\\pi\\) instead.] Let \\(Y=X+3\\): \\[ f_Y(y)=\\mbox{P}(Y=y)=\\mbox{P}(X+3=y)=\\mbox{P}(X=y-3)=f_X(y-3)=\\binom{n}{y-3}\\pi^{y-3}(1-\\pi)^{n-y+3} \\] where \\(3\\leq Y \\leq n+3\\). 16.2.3 Problem 3 3. Let \\(X\\sim \\textsf{Expon}(\\lambda)\\). Let \\(Y=X^2\\). Find the pdf of \\(Y\\). CDF method: \\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(X^2\\leq y)=\\mbox{P}(X\\leq \\sqrt y)=1-e^{-\\lambda \\sqrt y} \\] So, \\[ f_Y(y)=\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y}F_Y(y)=-e^{-\\lambda \\sqrt y}\\times\\frac{-\\lambda}{2\\sqrt y}=\\frac{\\lambda e^{-\\lambda \\sqrt y}}{2\\sqrt y} \\] for \\(y &gt;0\\). PDF method: \\[ f_Y(y)=\\lambda e^{-\\lambda \\sqrt{y}}\\frac{1}{2\\sqrt y}=\\frac{\\lambda e^{-\\lambda \\sqrt y}}{2\\sqrt y} \\] for \\(y&gt;0\\). 16.2.4 Problem 4 4. OPTIONAL: In exercise 3, you found the pdf of \\(Y=X^2\\) when \\(X\\sim \\textsf{Expon}(\\lambda)\\). Rearrange the pdf to show that \\(Y\\sim \\textsf{Weibull}\\) and find the parameters of that distribution. \\[ f_Y(y)=\\frac{\\lambda e^{-\\lambda \\sqrt y}}{2\\sqrt y}=\\frac{\\lambda}{2\\sqrt y}e^{-\\lambda \\sqrt y}=\\frac{\\lambda^2}{2} \\frac{1}{\\lambda\\sqrt y} e^{-\\sqrt{\\lambda^2 y}}=\\frac{1/2}{1/\\lambda^2}\\left(\\frac{y}{1/\\lambda^2}\\right)^{\\frac{1}{2}-1}e^{-\\left(\\frac{y}{1/\\lambda^2}\\right)^{\\frac{1}{2}}} \\] So, \\(Y\\sim \\textsf{Weibull}\\left(\\alpha=\\frac{1}{2},\\beta=\\frac{1}{\\lambda^2}\\right)\\). 16.2.5 Problem 5 5. You are on a team of two. You are both tasked to complete an exercise. The time it takes you \\(T_1\\), and likewise, your teammate \\(T_2\\) to complete the exercise are independent random variables. Exercise completion time, in minutes, is distributed with the following pdf: \\[ f_T(t)= \\frac{-t}{200}+\\frac{3}{20}; 10 \\leq t \\leq30 \\] Figure 16.1 is a plot of the pdf. Figure 16.1: pdf of \\(T\\) We want to find the probability our combined time is less than 40 minutes, \\(\\mbox{P}(T_1 + T_2 &lt; 40)\\). We will solve this in steps in this problem. We are going to use a computational method because the mathematics is long and algebra intensive. You are welcome to try a mathematical solution if you like but we will not provide a mathematical solution. Use the integrate() function to confirm this is a valid pdf. integrate(function(x)-x/200+3/20,10,30) ## 1 with absolute error &lt; 1.1e-14 Find the cdf of \\(T\\) mathematically. \\[ \\int_{10}^{x} \\frac{-t}{200}+\\frac{3}{20} \\,\\mathrm{d}t = \\frac{-t^2}{400}+\\frac{3t}{20}\\bigg|_{10}^x = \\frac{-x^2}{400}+\\frac{3x}{20} - \\frac{5}{4} \\] To use the cdf to simulate random variables from this distribution, we need the inverse of the cdf which means we have to solve a quadratic equation. We can do this mathematically or just use the function uniroot(). So first, we will make sure we understand how to use uniroot(). As a check, we know the median of the distribution is approximately 15.857. Here is code to show that 15.857 is approximately the median. We are integrating the pdf from 10 to 15.857 to confirm that this is 0.5. integrate(function(x)-x/200+3/20,10,15.857) ## 0.4999389 with absolute error &lt; 5.6e-15 Use uniroot() and your cdf to confirm that 15.857 is the median. Solution. uniroot(function(x)-x^2/400+3*x/20-5/4-.5,c(10,30))$root ## [1] 15.85786 We will create a function to take a random uniform variable on the interval \\([0,1]\\) and return a value of our random variable, \\(T\\), exercise time. We can then use this function to simulate each of the exercise times and then create a new random variable that is the sum. Complete the R code and check that it returns the median. T &lt;- function(y){ uniroot(function(x)&quot;YOUR CDF HERE as a function of x&quot;-y,c(10,30))$root } We made it a function of \\(y\\) since we are using \\(x\\) in our cdf. There are two function calls here, can you see why? T &lt;- function(y){ uniroot(function(x)-x^2/400+3*x/20-5/4-y,c(10,30))$root } T(.5) ## [1] 15.85786 Vectorize the function you just created using the Vectorize() function. Check that it is vectorized by entering c(.5,.75) into the function. You should get 15.85786 20.00000 as an output. T&lt;-Vectorize(T) T(c(.5,.75)) ## [1] 15.85786 20.00000 integrate(function(x)-x/200+3/20,10,20) ## 0.75 with absolute error &lt; 8.3e-15 We are ready. Lets create a data frame with 10000 simulation for our time and another 10000 for our teammates. Remember to set a seed. At this point it may be hard to remember what we have done. The function we created takes as input a vector of random number from a uniform distribution and then applies the inverse cdf to generate a random sample from our given pdf. T(runif(3)) ## [1] 11.93220 22.36525 10.08998 set.seed(1144) sim_exercise &lt;- tibble(T1=T(runif(10000)),T2=T(runif(10000))) Do a numerical summary of the data and plot a density plot of your exercise times to give us confidence that we simulated the process correctly. inspect(sim_exercise) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 T1 numeric 10.00066 12.69182 15.91884 20.04111 29.85565 16.69611 ## ...2 T2 numeric 10.00028 12.65653 15.79768 20.03195 29.63950 16.64766 ## sd n missing ## ...1 4.719835 10000 0 ## ...2 4.716582 10000 0 gf_dens(~T1,data=sim_exercise) %&gt;% gf_theme(theme_classic()) Create the new variable that is the sum of the two exercise time and then find the probability that the sum is less than 40. sim_exercise %&gt;% mutate(T_sum=T1+T2) %&gt;% summarise(prob=mean(T_sum&lt;40)) ## # A tibble: 1 x 1 ## prob ## &lt;dbl&gt; ## 1 0.835 "],["EST.html", "Chapter 17 Estimation Methods 17.1 Objectives 17.2 Homework", " Chapter 17 Estimation Methods 17.1 Objectives Obtain a method of moments estimate of a parameter or set of parameters. Given a random sample from a distribution, obtain the likelihood function. Obtain a maximum likelihood estimate of a parameter or set of parameters. 17.2 Homework 17.2.1 Problem 1 1. In the chapter notes, we found that if we take a sample from the uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), the method of moments estimate of \\(\\theta\\) is \\(\\hat{\\theta}_{MoM}=2\\bar{x}\\). Suppose our sample consists of the following values: \\[ 0.2 \\hspace{0.4cm} 0.9 \\hspace{0.4cm} 1.9 \\hspace{0.4cm} 2.2 \\hspace{0.4cm} 4.7 \\hspace{0.4cm} 5.1 \\] What is \\(\\hat{\\theta}_{MoM}\\) for this sample? x&lt;-c(0.2,0.9,1.9,2.2,4.7,5.1) thetamom&lt;-2*mean(x) thetamom ## [1] 5 What is wrong with this estimate? For our distribution \\(\\theta\\) is the upper bound or largest value for the random variable. The estimate for \\(\\theta\\) is 5, which is an impossible value for \\(\\theta\\), since one of our observations (5.1) is beyond this value. Show that this estimator is unbiased. We need to show that \\[ E\\left( \\hat{\\theta}_{MoM} \\right) = \\theta \\] We proceed as follows \\[ E \\left(2\\bar{X} \\right) = 2 E\\left( \\sum{\\frac{X_i}{n}} \\right) = \\frac{2}{n} E\\left( \\sum{X_i} \\right) \\] \\[ =\\frac{2}{n} \\sum{E\\left(X_i \\right)} =\\frac{2}{n}\\sum{\\frac{\\theta}{2}}=\\frac{n\\theta}{n}=\\theta \\] Notice that in performing this derivation, we treated \\(X\\) as a random variable and not as \\(x\\), a data value. ADVANCED: Use simulation in R to find out how often the method of moment estimator is less the maximum observed value, (\\(\\hat{\\theta}_{MoM} &lt; \\max x\\)). Report an answer for various sizes of samples. You can just pick an arbitrary value for \\(\\theta\\) when you sample from the uniform. However, the minimum must be 0. Lets start by writing code for one sample size and then generalize. This function will take as input the number of data points, sample that many points from a uniform with a max of 5, and then return a logical value comparing the method of moments estimate to the observed maximum. The choice of a maximum value for the uniform distribution is arbitrary. check &lt;- function(n=10){ temp&lt;-runif(n,max=5) 2*mean(temp)&lt;max(temp) } Lets test the function. set.seed(3030) temp&lt;-runif(10,max=5) temp ## [1] 1.048619 2.370056 0.675677 2.434443 2.994015 3.930865 4.708367 1.645806 ## [9] 2.592365 3.664424 2*mean(temp) ## [1] 5.212927 Reset the seed and run the function, we should get FALSE. set.seed(3030) check(10) ## [1] FALSE Now lets repeat the test 10000 find the proportion of times the method of moments estimator is unrealistic. (do(10000)*check(10)) %&gt;% summarize(mean(check)) %&gt;% pull() ## [1] 0.285 Lets make check a vectorized function for we can run for many sample sizes. check &lt;- Vectorize(check) Run 1000 replicates for each sample size. The rest of the code gets my data frame in the proper shape. my_data&lt;-(do(1000)*check(seq(10,200,5))) %&gt;% summarise_all(mean) %&gt;% pivot_longer(everything(),names_to = &quot;Sample&quot;,values_to = &quot;Percent&quot;) %&gt;% mutate(Sample=seq(10,200,5)) A quick look at the data. head(my_data) ## # A tibble: 6 x 2 ## Sample Percent ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10 0.282 ## 2 15 0.358 ## 3 20 0.362 ## 4 25 0.362 ## 5 30 0.398 ## 6 35 0.389 Now we can plot the results of sample size versus my_data %&gt;% gf_line(Percent~Sample,xlab=&quot;Sample Size&quot;,title=&quot;Percent When Estimator is Invalid&quot;) %&gt;% gf_theme(theme = theme_minimal()) Here is more traditional, old school R code. simn&lt;-function(n){ y&lt;-replicate(1000,{ x&lt;-runif(n) (2*mean(x))&lt;max(x) }) mean(y) } t&lt;-seq(10,200,4) persim&lt;-sapply(t,simn) plot(t,persim,type=&quot;l&quot;) 17.2.2 Problem 2 2. Let \\(x_1,x_2,...,x_n\\) be a simple random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MoM}\\). Recall that \\(\\mbox{E}(X)={1\\over \\lambda}\\). Setting this equal to the sample moment \\(\\bar{x}\\) and solving for \\(\\lambda\\) yields the method of moment estimator. Thus, \\[ \\hat{\\lambda}_{MoM}={1\\over \\bar{x}} \\] 17.2.3 Problem 3 3. Let \\(x_1,x_2,...,x_n\\) be an iid random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MLE}\\). Recall that \\[ f_X(x;\\lambda)=\\lambda e^{-\\lambda x} \\] So the likelihood function is: \\[ L(\\lambda;\\boldsymbol{x})=\\prod_{i=1}^n \\lambda e^{-\\lambda x_i}=\\lambda^n e^{-\\lambda\\sum x_i} \\] And the log-likelihood function is: \\[ l(\\lambda;\\boldsymbol{x})=n\\log \\lambda - \\lambda \\sum x_i \\] Taking the derivative with respect to \\(\\lambda\\) and setting equal to 0: \\[ {\\,\\mathrm{d}l(\\lambda;\\boldsymbol{x})\\over \\,\\mathrm{d}\\lambda}={n\\over \\lambda}-\\sum x_i =0 \\] Note that \\({\\,\\mathrm{d}^2 l(\\lambda;\\boldsymbol{x})\\over\\,\\mathrm{d}\\lambda^2}=-{n\\over \\lambda^2}&lt;0\\), so \\(l\\) is always concave down. Thus, any optimum found is a maximum. So, \\[ \\hat{\\lambda}_{MLE}={n\\over \\sum x_i}={1\\over \\bar{x}} \\] 17.2.4 Problem 4 4. It is mathematically difficult to determine if the estimators found in questions 2 and 3 are unbiased. Since the sample mean is in the denominator; mathematically we may have to work with the joint pdf. So instead, use simulation to get an sense of whether the method of moments estimator for the exponential distribution is unbaised. We need to sample data from an exponential and then compare the the reciprocal of the mean to the parameter. set.seed(630) 1/mean(rexp(1000,rate=10)) ## [1] 10.09422 This is close, maybe we just got lucky. Lets repeat many times. (do(1000)*(1/mean(rexp(1000,rate=10)))) %&gt;% gf_boxplot(~result) Looks like it has the potential to be unbiased. We would need to investigate other values for \\(\\lambda\\). 17.2.5 Problem 5 5. Find a maximum likelihood estimator for \\(\\theta\\) when \\(X\\sim\\textsf{Unif}(0,\\theta)\\). Compare this to the method of moments estimator we found. Hint: Do not take the derivative of the likelihood function. \\[ L(\\theta;\\boldsymbol{x})=\\frac{1}{\\theta^n}, \\hspace{0.5cm} \\mbox{only if all }x_i\\leq \\theta \\] A better way to write this is so as to see the answer is to let \\(M =max(x_i)\\), then: \\[ L(\\theta;\\boldsymbol{x})=\\left\\{\\begin{array}{ll} 0, &amp; \\theta &lt; M= max(x_i) \\\\ \\frac{1}{\\theta^n}, &amp; \\theta \\geq M = max(x_i) \\\\ \\end{array}\\right. \\] Figure 16.1 is a plot of this likelihood function and you can see that the maximum occurs at the maximum observed data point. Figure 16.1: A graph of the likelihood function Recall that \\(f(x_i;\\theta)={1\\over \\theta}\\) if \\(x_i\\in [0,\\theta]\\) and 0 elsewhere. And since the likelihood function is simply the product of these pdfs, if any \\(x_i\\) is beyond \\(\\theta\\), then the entire likelihood function is 0. You can picture \\(L\\) as a decreasing function of \\(\\theta\\), but remembering that \\(L\\) takes the value 0 if \\(\\theta\\) is smaller than at least one \\(x_i\\). Thus, \\(L\\) achieves its maximum at \\(\\theta=\\max x_i\\). This estimate is more intuitive than the method of moments estimate (\\(2\\bar{x}\\)). The method of moments estimate is sometimes not feasible. Meanwhile, the MLE (\\(\\max x_i\\)) is always feasible. "],["CS3.html", "Chapter 18 Case Study 18.1 Objectives 18.2 Homework", " Chapter 18 Case Study 18.1 Objectives Define and use properly in context all new terminology. Conduct a hypothesis test using a permutation test to include all 4 steps. 18.2 Homework 18.2.1 Problem 1 1. Side effects of Avandia Rosiglitazone is the active ingredient in the controversial type~2 diabetes medicine Avandia and has been linked to an increased risk of serious cardiovascular problems such as stroke, heart failure, and death. A common alternative treatment is pioglitazone, the active ingredient in a diabetes medicine called Actos. In a nationwide retrospective observational study of 227,571 Medicare beneficiaries aged 65 years or older, it was found that 2,593 of the 67,593 patients using rosiglitazone and 5,386 of the 159,978 using pioglitazone had serious cardiovascular problems. These data are summarized in the contingency table below. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Cardiovascular} &amp; \\textbf{problems} &amp; \\\\&amp; &amp; &amp; Yes &amp; No &amp; Total \\\\ &amp;\\hline \\textbf{Treatment} &amp; \\textit{Rosiglitazone} &amp; 2,593 &amp; 65,000 &amp; 67,593 \\\\ &amp; &amp; \\textit{Pioglitazone} &amp; 5,386 &amp; 154,592 &amp; 159,978\\\\ &amp;\\hline &amp;Total &amp; 7,979 &amp; 219,592 &amp; 227,571 \\end{array} \\] Determine if each of the following statements is true or false. If false, explain why. The reasoning may be wrong even if the statements conclusion is correct. In such cases, the statement should be considered false. Since more patients on pioglitazone had cardiovascular problems (5,386 vs. 2,593), we can conclude that the rate of cardiovascular problems for those on a pioglitazone treatment is higher. False. Instead of comparing counts, we should compare percentages. The data suggest that diabetic patients who are taking rosiglitazone are more likely to have cardiovascular problems since the rate of incidence was (2,593 / 67,593 = 0.038) 3.8% for patients on this treatment, while it was only (5,386 / 159,978 = 0.034) 3.4% for patients on pioglitazone. True. The fact that the rate of incidence is higher for the rosiglitazone group proves that rosiglitazone causes serious cardiovascular problems. False. We cannot infer a causal relationship from an association in an observational study. However, we can say the drug a person is on affects his risk in this case, as he chose that drug and his choice may be associated with other variables, which is why part (b) is true. The difference in these statements is subtle but important. Based on the information provided so far, we cannot tell if the difference between the rates of incidences is due to a relationship between the two variables or due to chance. True. 18.2.2 Problem 2 2. Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called was used to indicate whether or not the patient was alive at the end of the study. In the study, of the 34 patients in the control group, 4 were alive at the end of the study. Of the 69 patients in the treatment group, 24 were alive. The contingency table below summarizes these results. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Group} &amp; &amp; \\\\&amp; &amp; &amp; Control &amp; Treatment &amp; Total \\\\ &amp;\\hline \\textbf{Outcome} &amp; \\textit{Alive} &amp; 4 &amp; 24 &amp; 28 \\\\ &amp; &amp; \\textit{Dead} &amp; 30 &amp; 45 &amp; 75\\\\ &amp;\\hline &amp;Total &amp; 34 &amp; 69 &amp; 103 \\end{array} \\] The data is in a file called Stanford_heart_study.csv. Read the data in and answer the following questions. heart&lt;-read_csv(&quot;data/Stanford_heart_study.csv&quot;) What proportion of patients in the treatment group and what proportion of patients in the control group died? inspect(heart) ## ## categorical variables: ## name class levels n missing ## 1 outcome character 2 103 0 ## 2 group character 2 103 0 ## distribution ## 1 Dead (72.8%), Alive (27.2%) ## 2 Treatment (67%), Control (33%) tally(~outcome+group,data=heart,margins = TRUE) ## group ## outcome Control Treatment Total ## Alive 4 24 28 ## Dead 30 45 75 ## Total 34 69 103 tally(outcome~group,data=heart,margins = TRUE,format=&quot;percent&quot;) ## group ## outcome Control Treatment ## Alive 11.76471 34.78261 ## Dead 88.23529 65.21739 ## Total 100.00000 100.00000 So 88.2% of patients in control group died and 65.2% in the treatment group. One approach for investigating whether or not the treatment is effective is to use a randomization technique. What are the claims being tested? Use the same null and alternative hypothesis notation used in the lesson notes. \\(H_0\\): Independence model. The variables group and outcome are independent. They have no relationship, and the difference in survival rates between the control and treatment groups was due to chance. In other words, heart transplant is not effective. \\(H_A\\): Alternative hypothesis. The variables group and outcome are not independent. The difference in survival rates between the control and treatment groups was not due to chance and the heart transplant is effective. The paragraph below describes the set up for such approach, if we were to do it without using statistical software. Fill in the blanks with a number or phrase, whichever is appropriate. We write alive on 28 cards representing patients who were alive at the end of the study, and dead on 75 cards representing patients who were not. Then, we shuffle these cards and split them into two groups: one group of size 69 representing treatment, and another group of size 34 representing control. We calculate the difference between the proportion of dead cards in the control and treatment groups (control - treatment), this is just so we have positive observed value, and record this value. We repeat this many times to build a distribution centered at zero. Lastly, we calculate the fraction of simulations where the simulated differences in proportions are 0.23 or greater. If this fraction of simulations, the empirical p-value, is low, we conclude that it is unlikely to have observed such an outcome by chance and that the null hypothesis should be rejected in favor of the alternative. Next we will perform the simulation and use results to decide the effectiveness of the transplant program. Find observed value of the test statistic, which we decided to use the difference in proportions. obs&lt;-diffprop(outcome~group,data=heart) obs ## diffprop ## 0.230179 Simulate 1000 values of the test statistic by using shuffle() on the variable group. set.seed(1213) results &lt;- do(1000)*diffprop(outcome~shuffle(group),data=heart) Plot distribution of results. Include a vertical line for the observed value. Clean up the plot as if you were presenting to a decision maker. results %&gt;% gf_histogram(~diffprop,xlab=&quot;Difference in proportions&quot;, ylab=&quot;Count&quot;, fill=&quot;cyan&quot;, color=&quot;black&quot;, title=&quot;Stanford Heart Study&quot;, subtitle=&quot;Distribution of difference between control and treatment groups&quot;) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_classic()) Find p-value. Think carefully about what more extreme would mean. results %&gt;% summarise(p_value = mean(diffprop&gt;=obs)) ## p_value ## 1 0.011 Decide if the treatment is effective. Under the independence model, only 11 out of 1000 times (1.1%) did we get a difference of 0.23 or higher between the proportions of patients that died in the control and treatment groups. Since this is a low probability, we can reject the claim of independence in favor of the alternate model. There is convincing evidence to suggest that the transplant program is defective. "],["HYPOTEST.html", "Chapter 19 Hypothesis Testing 19.1 Objectives 19.2 Homework", " Chapter 19 Hypothesis Testing 19.1 Objectives Know and properly use the terminology of a hypothesis test. Conduct all four steps of a hypothesis test using randomization. Discuss and explain the ideas of decision errors, one-sided versus two-sided, and choice of statistical significance. 19.2 Homework 19.2.1 Problem 1 1. Repeat the analysis of the commercial length in the notes. This time use a different test statistic. ads&lt;-read_csv(&quot;data/ads.csv&quot;) ads ## # A tibble: 10 x 2 ## basic premium ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6.95 3.38 ## 2 10.0 7.8 ## 3 10.6 9.42 ## 4 10.2 4.66 ## 5 8.58 5.36 ## 6 7.62 7.63 ## 7 8.23 4.95 ## 8 10.4 8.01 ## 9 11.0 7.8 ## 10 8.52 9.58 ads &lt;- ads %&gt;% pivot_longer(cols=everything(),names_to=&quot;channel&quot;,values_to = &quot;length&quot;) ads ## # A tibble: 20 x 2 ## channel length ## &lt;chr&gt; &lt;dbl&gt; ## 1 basic 6.95 ## 2 premium 3.38 ## 3 basic 10.0 ## 4 premium 7.8 ## 5 basic 10.6 ## 6 premium 9.42 ## 7 basic 10.2 ## 8 premium 4.66 ## 9 basic 8.58 ## 10 premium 5.36 ## 11 basic 7.62 ## 12 premium 7.63 ## 13 basic 8.23 ## 14 premium 4.95 ## 15 basic 10.4 ## 16 premium 8.01 ## 17 basic 11.0 ## 18 premium 7.8 ## 19 basic 8.52 ## 20 premium 9.58 favstats(length~channel,data=ads) ## channel min Q1 median Q3 max mean sd n missing ## 1 basic 6.950 8.30375 9.298 10.30000 11.016 9.2051 1.396126 10 0 ## 2 premium 3.383 5.05250 7.715 7.95975 9.580 6.8592 2.119976 10 0 State the null and alternative hypotheses. \\(H_0\\): Null hypothesis. The distribution of length of commercials in premium and basic channels is the same. \\(H_A\\): Alternative hypothesis. The distribution of length of commercials in premium and basic channels is different. Compute a test statistic. We will use the difference in means so we can use diffmeans() from mosiac. obs &lt;- diffmean(length~channel,data=ads) obs ## diffmean ## -2.3459 Determine the p-value. set.seed(4172) results &lt;- do(10000)*diffmean(length~shuffle(channel),data=ads) Next we create a plot of the empirical sampling distribution of the difference of means. results %&gt;% gf_histogram(~diffmean, fill=&quot;cyan&quot;, color=&quot;black&quot;) %&gt;% gf_vline(xintercept =obs) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Again, notice it is centered at zero and symmetrical. prop1(~(diffmean&lt;=obs),data=results) ## prop_TRUE ## 0.00459954 The p-value is much smaller! The test statistic matters in terms of efficiency of the testing procedure. Draw a conclusion. Based on our data, if there were really no difference in the distribution of lengths of commercials in 30 minute shows between basic and premium channels then the probability of finding our observed difference of means is 0.005. Since this is less than our significance level of 0.05, we reject the null in favor of the alternative that the basic channel has longer commercials. 19.2.2 Problem 2 2. Is yawning contagious? An experiment conducted by the MythBusters, a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasnt a person yawning near them (control). The following table shows the results of this experiment. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Group} &amp; &amp; \\\\&amp; &amp; &amp; Treatment &amp; Control &amp; Total \\\\ &amp;\\hline \\textbf{Result} &amp; \\textit{Yawn} &amp; 10 &amp; 4 &amp; 14 \\\\ &amp; &amp; \\textit{No Yawn} &amp; 24 &amp; 12 &amp; 36\\\\ &amp;\\hline &amp;Total &amp; 34 &amp; 16 &amp; 50 \\end{array} \\] The data is in the file yawn.csv. yawn &lt;- read_csv(&quot;data/yawn.csv&quot;) glimpse(yawn) ## Rows: 50 ## Columns: 2 ## $ group &lt;chr&gt; &quot;treatment&quot;, &quot;treatment&quot;, &quot;control&quot;, &quot;treatment&quot;, &quot;treatmen... ## $ outcome &lt;chr&gt; &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;yaw... inspect(yawn) ## ## categorical variables: ## name class levels n missing ## 1 group character 2 50 0 ## 2 outcome character 2 50 0 ## distribution ## 1 treatment (68%), control (32%) ## 2 no_yawn (72%), yawn (28%) tally(outcome~group,data=yawn,margins = TRUE,format=&quot;proportion&quot;) ## group ## outcome control treatment ## no_yawn 0.7500000 0.7058824 ## yawn 0.2500000 0.2941176 ## Total 1.0000000 1.0000000 What are the hypotheses? \\(H_0\\): Yawning is not contagious, someone in the group yawning does not impact the percentage of the group that yawns. \\(p_c - p_t = 0\\) or equivalently \\(p_c = p_t\\) . \\(H_A\\): Yawning does have an impact, it is contagious. If someone yawns then you are more likely to yawn. \\(p_t &gt; p_c\\) or \\(p_c - p_t &lt; 0\\). Calculate the observed difference between the yawning rates under the two scenarios. Yes we are giving you the test statistic. obs &lt;- diffprop(outcome~group,data=yawn) obs ## diffprop ## -0.04411765 Notice that it is negative. If it had been positive, then we would not even need the next step; we would fail to reject the null because the p-value would be much larger than 0.05. Think about this and make sure you understand. Estimate the p-value using randomization. set.seed(56) results&lt;-do(10000)*diffprop(outcome~shuffle(group),data=yawn) prop1(~(diffprop&lt;=obs),data=results) ## prop_TRUE ## 0.5140486 This is a large p-value. Plot the empirical sampling distribution. results %&gt;% gf_histogram(~diffprop, fill=&quot;cyan&quot;, color=&quot;black&quot;) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Determine the conclusion of the hypothesis test. Since p-value, 0.54, is high, larger than 0.05, we fail to reject the null hypothesis of yawning is not contagious. The data do not provide convincing evidence that people are more likely to yawn if a person near them yawns. The traditional belief is that yawning is contagious  one yawn can lead to another yawn, which might lead to another, and so on. In this exercise, there was the option of selecting a one-sided or two-sided test. Which would you recommend (or which did you choose)? Justify your answer in 1-3 sentences. I chose an one-sided test since as a researcher, I thought having someone in the group yawn would lead to more people in that group yawning. How did you select your level of significance? Explain in 1-3 sentences. Since there was no clear impact on one type of error being worse than the other, I stayed with the default of 0.05. "],["references.html", "References", " References "]]
