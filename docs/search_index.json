[["index.html", "Solutions for Computational Probability and Statistics Preface 0.1 Book Structure and How to Use It 0.2 Packages 0.3 File Creation Information", " Solutions for Computational Probability and Statistics Ken Horton Kris Pruitt Bradley Warner 2021-03-09 Preface Contained in this volume are the solutions to homework problems in the Computational Probability and Statistics book. 0.1 Book Structure and How to Use It This solution manual is setup to match the structure of the accompanying book. The learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for: Developing probabilistic models Developing statistical models for inference and description Advancing practical and theoretical analytic experience and skills 0.2 Packages These notes make use of the following packages in R knitr (Xie 2020), rmarkdown (Allaire et al. 2020), mosaic (Pruim, Kaplan, and Horton 2020), mosaicCalc (Kaplan, Pruim, and Horton 2020), tidyverse (Wickham 2019), ISLR (James et al. 2017), vcd (Meyer, Zeileis, and Hornik 2020), ggplot2 (Wickham et al. 2020), MASS (Ripley 2019), openintro (Ã‡etinkaya-Rundel et al. 2020), broom (Robinson, Hayes, and Couch 2020), infer (Bray et al. 2020), ISLR (James et al. 2017), kableExtra (Zhu 2020), DT (Xie, Cheng, and Tan 2020). This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 0.3 File Creation Information File creation date: 2021-03-09 Windows version: Windows 10 x64 (build 18362) R version 3.6.3 (2020-02-29) References "],["CS1.html", "Chapter 1 Case Study 1.1 Objectives 1.2 Homework", " Chapter 1 Case Study 1.1 Objectives Use R for basic analysis and visualization. Compile a report using knitr. 1.2 Homework Load tidyverse,mosaic, and knitr packages. library(tidyverse) library(mosaic) library(knitr) 1.2.1 Problem 1 Stent study continued. Complete a similar analysis for the stent data but this time for the one year data. In particular Read the data into your working directory. stent_study &lt;-read_csv(&#39;data/stent_study.csv&#39;) Complete similar steps as in the class notes. i. Use inspect on the data. ii. Create a table of outcome365 and group. Comment on the results. iii. Create a barchart of the data. inspect(stent_study) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## ## categorical variables: ## name class levels n missing ## 1 group character 2 451 0 ## 2 outcome30 character 2 451 0 ## 3 outcome365 character 2 451 0 ## distribution ## 1 control (50.3%), trmt (49.7%) ## 2 no_event (89.8%), stroke (10.2%) ## 3 no_event (83.8%), stroke (16.2%) tally(outcome365~group,data=stent_study,format=&quot;proportion&quot;,margins = TRUE) ## group ## outcome365 control trmt ## no_event 0.8766520 0.7991071 ## stroke 0.1233480 0.2008929 ## Total 1.0000000 1.0000000 Patients in the treatment group had a higher proportion of strokes than those in the control group after one year. The treatment does not appear to help the rate of strokes and in fact may hurt it. stent_study %&gt;% gf_props(~group,fill=~outcome365,position=&#39;fill&#39;) %&gt;% gf_labs(title=&quot;Impact of Stents of Stroke&quot;, subtitle=&#39;Experiment with 451 Patients&#39;, x=&quot;Experimental Group&quot;, y=&quot;Number of Events&quot;) 1.2.2 Problem 2 Migraine and acupuncture. A migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. 46 patients in the control group received placebo acupuncture (needle insertion at nonacupoint locations). 24 hours after patients received acupuncture, they were asked if they were pain free.1 The data is in the file migraine_study.csv in the folder data. Complete the following work: Read the data an object called migraine_study. migraine_study &lt;- read_csv(&quot;data/migraine_study.csv&quot;) head(migraine_study) ## # A tibble: 6 x 2 ## group pain_free ## &lt;chr&gt; &lt;chr&gt; ## 1 treatment yes ## 2 treatment yes ## 3 treatment yes ## 4 treatment yes ## 5 treatment yes ## 6 treatment yes Create a table of the data. tally(pain_free~group,data=migraine_study,format=&quot;proportion&quot;,margin=TRUE) ## group ## pain_free control treatment ## no 0.95652174 0.76744186 ## yes 0.04347826 0.23255814 ## Total 1.00000000 1.00000000 Report the percent of patients in the treatment group who were pain free 24 hours after receiving acupuncture. There are 23.2% of the treatment group pain free. Repeat for the control group. There are only 4.3% of the control group pain free. At first glance, does acupuncture appear to be an effective treatment for migraines? Explain your reasoning. Yes, a substantial increase in the percentage of patients pain free after acupuncture versus those with no acupuncture, so it appears to be effective. Do the data provide convincing evidence that there is a real pain reduction for those patients in the treatment group? Or do you think that the observed difference might just be due to chance? Either of these is acceptable: i. We could get slightly different group estimates even if there is no real difference. Though the difference is big, Im skeptical the results show a real difference and think this might be due to chance. ii. The difference in these rates looks pretty big, and so I suspect acupuncture is having a positive impact on pain. Compile, knit, this report into a pdf. Complete on your computer or server. G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173175. "],["DB.html", "Chapter 2 Data Basics 2.1 Objectives 2.2 Homework", " Chapter 2 Data Basics 2.1 Objectives Define and use properly in context all new terminology to include but not limited to case, observational unit, variables, data frame, associated variables, independent, and discrete and continuous variables. Identify and define the different types of variables. From reading a study, explain the research question. Create a scatterplot in R and determine the association of two numerical variables from the plot. 2.2 Homework Identify study components Identify (i) the cases, (ii) the variables and their types, and (iii) the main research question in the studies described below. 2.2.1 Problem 1 Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.2 The cases are 143,196 eligible study subjects who were born in Southern California between 1989 and 1993. The variables are measurements of carbon monoxide (CO), nitrogen dioxide, ozone, and particulate matter less than 10\\(\\mu m\\) (PM10) collected at air-quality-monitoring stations as well as length of gestation. All of these variables are continuous numerical variables. The research question was Is there an association between air pollution exposure and preterm births? 2.2.2 Problem 2 The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.3 The cases are 600 adult patients aged 18-69 years diagnosed and currently treated for asthma. The variables were whether or not the patient practiced the Buteyko method (categorical) and measures of quality of life, activity, asthma symptoms and medication reduction of the patients (categorical, ordinal). It may also be reasonable to treat the ratings on a scale of 1 to 10 as discrete numerical variables. The research question was Do asthmatic patients who practice the Buteyko method experience improvement in their condition? B. Ritz et al. Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993. In: Epidemiology 11.5 (2000), pp. 502511. J. McGowan. Health Education: Does the Buteyko Institute Method make a difference? In: Thorax 58 (2003). "],["ODCP.html", "Chapter 3 Overview of Data Collection Principles 3.1 Objectives 3.2 Homework", " Chapter 3 Overview of Data Collection Principles 3.1 Objectives Define and use properly in context all new terminology. From a description of a research project, at a minimum be able to describe the population of interest, the generalizability of the study, the response and predictor variables, differentiate whether it is observational or experimental, and determine the type of sample. 3.2 Homework 3.2.1 Problem 1 Generalizability and causality. Identify the population of interest and the sample in the studies described below, these are the same studies from the prevous lesson. Also comment on whether or not the results of the study can be generalized to the population and if the findings of the study can be used to establish causal relationships. Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.4 The population of interest is all births. The sample consists of the 143,196 births between 1989 and 1993 in Southern California. If births in this time span at the geography can be considered to be representative of all births, then the results are generalizable to the population of Southern California. However, since the study is observational the findings cannot be used to establish causal relationships. The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.5 The population is all 18-69 year olds diagnosed and currently treated for asthma. The sample is the 600 adult patients aged 18-69 years diagnosed and currently treated for asthma. Since the sample is not random (voluntary) the results cannot be generalized to the population at large. However, since the study is an experiment, the findings can be used to establish causal relationships. 3.2.2 Problem 2 GPA and study time. A survey was conducted on 55 undergraduates from Duke University who took an introductory statistics course in Spring 2012. Among many other questions, this survey asked them about their GPA and the number of hours they spent studying per week. The scatterplot below displays the relationship between these two variables. What is the explanatory variable and what is the response variable? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Is this an experiment or an observational study? Can we conclude that studying longer hours leads to higher GPAs? Solutions The explanatory variable is the number of study hours per week, and the response variable is GPA. There is a somewhat weak positive relationship between the two variables, though the data become more sparse as the number of study hours increases. One responded reported a GPA above 4.0, which is clearly a data error. Also, there are a few respondents who reported unusually high study hours (60 and 70 hours/week). It should also be noted that the variability in GPA is much higher for students who study less than those who study more, also might be due to the fact that there arent many respondents who reported studying higher hours. This is an observational study. Since this is an observational study, we cannot conclude that there is a causal relationship between the two variables even though there appears to be an association. 3.2.3 Problem 3 Income and education The scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelors degree in 3,143 counties in the US in 2010. What are the explanatory and response variables? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Can we conclude that having a bachelors degree increases ones income? Solutions The explanatory variable is percent of population with a bachelors degree and the response variable is per capita income (in thousands). There is a strong positive linear relationship between the two variables. As the percentage of population with a bachelors degree increases the per capita income increases as well. There are very few counties where more than 60% of the population have a bachelors degree and very few countries that have a more than $50,000 in per capita income. This is an observational study so we cannot make a causal statement based on the results. However, we can say that having a higher percentage of population with bachelors degree is associated with a higher per capita income. B. Ritz et al. Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993. In: Epidemiology 11.5 (2000), pp. 502511. J. McGowan. Health Education: Does the Buteyko Institute Method make a difference? In: Thorax 58 (2003). "],["STUDY.html", "Chapter 4 Studies 4.1 Objectives 4.2 Homework", " Chapter 4 Studies 4.1 Objectives Define and use properly in context all new terminology. Given a study description, be able to identify and explain the study using correct terms. Given a scenario, describe flaws in reasoning and propose study and sampling designs. 4.2 Homework 4.2.1 Problem 1 Propose a sampling strategy. A large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each of 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the students overall satisfaction with the course. What type of study is this? Observational study. Suggest a sampling strategy for carrying out this study. Stratified sample, sample randomly within each section. 4.2.2 Problem 2 Flawed reasoning. Identify the flaw in reasoning in the following scenarios. Explain what the individuals in the study should have done differently if they wanted to make such strong conclusions. Students at an elementary school are given a questionnaire that they are required to return after their parents have completed it. One of the questions asked is, Do you find that your work schedule makes it difficult for you to spend time with your kids after school? Of the parents who replied, 85% said no. Based on these results, the school officials conclude that a great majority of the parents have no difficulty spending time with their kids after school. Solution Non-responders may have a different response to this question. The parents who returned the surveys are probably those who do not have difficulty spending time with their kids after school. Parents who work might not have returned the surveys since they probably have a busier schedule. A survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. A follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. The researcher reports that these 567 women are representative of all mothers. Solution It is unlikely that the women who were reached at the same address 3 years later are a random sample. These missing responders are probably renters (as opposed to homeowners) which means that they might be in a lower socio-economic status than the respondents. 4.2.3 Problem 3 Sampling strategies. A Math 377 student who is curious about the relationship between the amount of time students spend on social networking sites and their performance at school decides to conduct a survey. Four research strategies for collecting data are described below. In each, name the sampling method proposed and any bias you might expect. He randomly samples 40 students from the studys population, gives them the survey, asks them to fill it out and bring it back the next day. He gives out the survey only to his friends, and makes sure each one of them fills out the survey. He posts a link to an online survey on his Facebook wall and asks his friends to fill out the survey. He stands outside the QRC and asks every third person that walks out the door to fill out the survey. Solution a. Simple random sample. Non-response bias, if only those people who have strong opinions about the survey responds his sample may not be representative of the population. b. Convenience sample. Under coverage bias, his sample may not be representative of the population since it consists only of his friends. It is also possible that the study will have non-response bias if some choose to not bring back the survey. c. Convenience sample. This will have a similar issues to handing out surveys to friends. d. Convenience sample. Same. 4.2.4 Problem 4 Vitamin supplements. In order to assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No significant differences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms. Was this an experiment or an observational study? Why? What are the explanatory and response variables in this study? Were the patients blinded to their treatment? Was this study double-blind? Participants are ultimately able to choose whether or not to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning. Solution a. Experiment, since the researchers randomly assigned different treatments to the participants. b. Response variable: Duration of the cold. Explanatory variable: Treatment, with 4 levels; placebo, 1g, 3g, 3g with additives. c. The patients were blinded as they did not know which treatment they received. d. The study was double-blind with respect to the researchers evaluating the patients, but the nurses who briely interacted with patients during the distribution of the medication were not blinded. (It was partially double-blind.) e. Since the patients were randomly assigned to the treatment groups and they are blinded we would expect about an equal number of patients in each group to not adhere to the treatment. While this means that final results of the study will be based on fewer number of participants, non-adherence does not introduce a confounding variable to the study. 4.2.5 Problem 5 Exercise and mental health. A researcher is interested in the effects of exercise on mental health and she proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results. What type of study is this? What are the treatment and control groups in this study? Does this study make use of blocking? If so, what is the blocking variable? Does this study make use of blinding? Comment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large. Suppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal? Solution a. This is an experiment since we assigned subjects to the exercise program. b. The treatment is exercise twice a week and control is no exercise. c, Yes, the blocking variable is age. d. No, the study is not blinded since the patients will know whether or not they are exercising. e. Since this is an experiment, we can make a causal statement. Since the sample is random, the causal statement can be generalized to the population at large. However, we should be cautious about making a causal statement because of a possible placebo effect. f. It would be very difficult, if not impossible, to successfully conduct this study since randomly sampled people cannot be required to participate in a clinical trial. "],["NUMDATA.html", "Chapter 5 Numerical Data 5.1 Objectives 5.2 Homework", " Chapter 5 Numerical Data 5.1 Objectives Define and use properly in context all new terminology. Generate in R summary statistics for a numeric variable including breaking down by cases. Generate in R appropriate graphical summaries of numerical variables. Be able to interpret and explain output both graphically and numerically. 5.2 Homework 5.2.1 Problem 1 Mammals exploratory Data were collected on 39 species of mammals distributed over 13 orders. The data is in the openintro package as mammals Using help, report the units for the variable BrainWt. ?mammals Using inspect how many variables are numeric? inspect(mammals) ## ## categorical variables: ## name class levels n missing ## 1 species factor 62 62 0 ## distribution ## 1 Africanelephant (1.6%) ... ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 body_wt numeric 0.005 0.600 3.3425 48.2025 6654.0 198.789984 ## ...2 brain_wt numeric 0.140 4.250 17.2500 166.0000 5712.0 283.134194 ## ...3 non_dreaming numeric 2.100 6.250 8.3500 11.0000 17.9 8.672917 ## ...4 dreaming numeric 0.000 0.900 1.8000 2.5500 6.6 1.972000 ## ...5 total_sleep numeric 2.600 8.050 10.4500 13.2000 19.9 10.532759 ## ...6 life_span numeric 2.000 6.625 15.1000 27.7500 100.0 19.877586 ## ...7 gestation numeric 12.000 35.750 79.0000 207.5000 645.0 142.353448 ## ...8 predation integer 1.000 2.000 3.0000 4.0000 5.0 2.870968 ## ...9 exposure integer 1.000 1.000 2.0000 4.0000 5.0 2.419355 ## ...10 danger integer 1.000 1.000 2.0000 4.0000 5.0 2.612903 ## sd n missing ## ...1 899.158011 62 0 ## ...2 930.278942 62 0 ## ...3 3.666452 48 14 ## ...4 1.442651 50 12 ## ...5 4.606760 58 4 ## ...6 18.206255 58 4 ## ...7 146.805039 58 4 ## ...8 1.476414 62 0 ## ...9 1.604792 62 0 ## ...10 1.441252 62 0 What type of variable is danger? Categorical Create a histogram of total_sleep and describe the distribution. gf_histogram(~total_sleep,data=mammals,binwidth = 2) gf_dens(~total_sleep,data=mammals) The distribution is unimodal and skewed to the right. It appears it is centered around the value of 11. Create a boxplot of life_span and describe the distribution. gf_boxplot(~life_span,data=mammals) Report the mean and median life span of a mammal. mean(~life_span,data=mammals,na.rm=TRUE) ## [1] 19.87759 median(~life_span,data=mammals,na.rm=TRUE) ## [1] 15.1 Calculate the summary statistics for LifeSpan broken down by Danger. favstats(life_span~danger,data=mammals) ## danger min Q1 median Q3 max mean sd n missing ## 1 1 3.0 7.700 17.60 32.500 100.0 24.20556 23.53829 18 1 ## 2 2 2.3 4.500 10.40 13.000 50.0 12.92308 13.15948 13 1 ## 3 3 2.0 4.175 5.35 7.875 38.6 9.43750 11.99559 8 2 ## 4 4 2.6 9.775 22.10 27.000 69.0 23.11000 18.75482 10 0 ## 5 5 17.0 20.000 23.60 30.000 46.0 26.95556 10.18910 9 0 5.2.2 Problem 2 Mammals life spans Continue using the mammals data set. Create side-by-side boxplots for life_span broken down by exposure. Note: you will have to change exposure to a factor(). Report on any findings. mammals %&gt;% gf_boxplot(life_span~factor(exposure)) What happened to the median and third quartile in exposure group 4? favstats(life_span~factor(exposure),data=mammals) ## factor(exposure) min Q1 median Q3 max mean sd n missing ## 1 1 2.0 4.35 7.25 14.550 100.0 14.55000 20.98594 24 3 ## 2 2 2.3 6.00 11.20 17.275 50.0 15.39167 14.55819 12 1 ## 3 3 7.6 19.90 26.50 32.000 41.0 25.40000 13.84582 4 0 ## 4 4 7.0 20.20 27.00 27.000 39.3 24.10000 11.78431 5 0 ## 5 5 16.3 20.00 28.00 38.600 69.0 30.53077 14.98084 13 0 Create faceted histograms. What are the shortcomings of this plot? gf_histogram(~life_span,color=~factor(exposure),data=mammals) This is awful. gf_histogram(~life_span|factor(exposure),data=mammals) Not enough data for each histogram; some of the histograms provide little to no information. Lets do denisty plots. gf_dens(~life_span,color=~factor(exposure),data=mammals) gf_dens(~life_span|factor(exposure),data=mammals) Which do you think is the best graph? Create a new variable exposed that is a factor with level Low if exposure is 1 or 2 and High otherwise. mammals &lt;- mammals %&gt;% mutate(exposed=factor(ifelse((exposure==1)|(exposure==2),&quot;Low&quot;,&quot;High&quot;))) inspect(mammals) ## ## categorical variables: ## name class levels n missing ## 1 species factor 62 62 0 ## 2 exposed factor 2 62 0 ## distribution ## 1 Africanelephant (1.6%) ... ## 2 Low (64.5%), High (35.5%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 body_wt numeric 0.005 0.600 3.3425 48.2025 6654.0 198.789984 ## ...2 brain_wt numeric 0.140 4.250 17.2500 166.0000 5712.0 283.134194 ## ...3 non_dreaming numeric 2.100 6.250 8.3500 11.0000 17.9 8.672917 ## ...4 dreaming numeric 0.000 0.900 1.8000 2.5500 6.6 1.972000 ## ...5 total_sleep numeric 2.600 8.050 10.4500 13.2000 19.9 10.532759 ## ...6 life_span numeric 2.000 6.625 15.1000 27.7500 100.0 19.877586 ## ...7 gestation numeric 12.000 35.750 79.0000 207.5000 645.0 142.353448 ## ...8 predation integer 1.000 2.000 3.0000 4.0000 5.0 2.870968 ## ...9 exposure integer 1.000 1.000 2.0000 4.0000 5.0 2.419355 ## ...10 danger integer 1.000 1.000 2.0000 4.0000 5.0 2.612903 ## sd n missing ## ...1 899.158011 62 0 ## ...2 930.278942 62 0 ## ...3 3.666452 48 14 ## ...4 1.442651 50 12 ## ...5 4.606760 58 4 ## ...6 18.206255 58 4 ## ...7 146.805039 58 4 ## ...8 1.476414 62 0 ## ...9 1.604792 62 0 ## ...10 1.441252 62 0 Repeat part c with the new variable. gf_dens(~life_span,color=~exposed,data=mammals) 5.2.3 Problem 3 Mammals life spans continued Create a scatterplot of life span versus length of gestation. mammals %&gt;% gf_point(life_span~gestation) What type of an association is apparent between life span and length of gestation? It is a weak positive association. What type of an association would you expect to see if the axes of the plot were reversed, i.e. if we plotted length of gestation versus life span? The same as this is observational data there is no reason to beliee is a causal relationship just by looking at the data. Switching the axis will preserve the association. Create the new scatterplot suggested in c. mammals %&gt;% gf_point(gestation~life_span) Are life span and length of gestation independent? Explain your reasoning. No there is an association and it appears to be linear. If the plot looked like a shotgun blast, we would consider the variables to be independent. However, remember there may be confounding variables that could impact the association between these variables. "],["CATDATA.html", "Chapter 6 Categorical Data 6.1 Objectives 6.2 Homework", " Chapter 6 Categorical Data 6.1 Objectives Define and use properly in context all new terminology. Generate in R tables for categorical variable(s). Generate in R appropriate graphical summaries of categorical and numerical variables. Be able to interpret and explain output both graphically and numerically. 6.2 Homework Make sure your plots have a title and the axes are labeled. 6.2.1 Problem 1 Views on immigration 910 randomly sampled registered voters from Tampa, FL were asked if they thought workers who have illegally entered the US should be (i) allowed to keep their jobs and apply for US citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for US citizenship, or (iii) lose their jobs and have to leave the country. The data is in the openintro package in the immigration data object. How many levels of political are there? levels(immigration$political) ## [1] &quot;conservative&quot; &quot;liberal&quot; &quot;moderate&quot; inspect(immigration) ## ## categorical variables: ## name class levels n missing ## 1 response factor 4 910 0 ## 2 political factor 3 910 0 ## distribution ## 1 Leave the country (38.5%) ... ## 2 conservative (40.9%), moderate (39.9%) ... There are three levels for political and they are conservative, liberal, and moderate. Create a table using tally. round(tally(~response+political,data=immigration,format=&quot;percent&quot;,margins = TRUE),2) ## political ## response conservative liberal moderate Total ## Apply for citizenship 6.26 11.10 13.19 30.55 ## Guest worker 13.30 3.08 12.42 28.79 ## Leave the country 19.67 4.95 13.85 38.46 ## Not sure 1.65 0.11 0.44 2.20 ## Total 40.88 19.23 39.89 100.00 What percent of these Tampa, FL voters identify themselves as conservatives? From the table, 40.88% of voters identified themselves as conservatives. What percent of these Tampa, FL voters are in favor of the citizenship option? Again, from the table 30.55% of the voters favor the citizenship option. What percent of these Tampa, FL voters identify themselves as conservatives and are in favor of the citizenship option? From the table, 6.26% of the voters are conservative and favor the citizenship option. What percent of these Tampa, FL voters who identify themselves as conservatives are also in favor of the citizenship option? What percent of moderates and liberal share this view? We need a different table for this question. round(tally(response~political,data=immigration,format=&quot;percent&quot;,margins = TRUE),2) ## political ## response conservative liberal moderate ## Apply for citizenship 15.32 57.71 33.06 ## Guest worker 32.53 16.00 31.13 ## Leave the country 48.12 25.71 34.71 ## Not sure 4.03 0.57 1.10 ## Total 100.00 100.00 100.00 Of the conservative voters, 15.32% are in favor of the citizenship option. The numbers are 57.71% for liberals and 33.06% for moderates. Create a stacked bar chart. immigration %&gt;% gf_props(~political,fill=~response,position=&quot;fill&quot;) %&gt;% gf_labs(title=&quot;Tampa Florida Voter Views on Illegal Immigrant Workers&quot;, subtitle=&quot;Broken down by political views&quot;,x=&quot;Political View&quot;,y=&quot;Proportion&quot;) %&gt;% gf_theme(theme_bw()) Using your plot, do political ideology and views on immigration appear to be independent? Explain your reasoning. The percentages of Tampa, FL conservatives, moderates, and liberals who are in favor of illegal immigrants working in the US staying and applying for citizenship are quite different from one another. Therefore, the two variables appear to be dependent. 6.2.2 Problem 2 Views on the DREAM Act The same survey from Exercise 1 also asked respondents if they support the DREAM Act, a proposed law which would provide a path to citizenship for people brought illegally to the US as children. The data is in the openintro package in the dream data object. Create a mosaic plot. mosaic(stance~ideology,data=dream,sub=&quot;Voter views on illegal worker status&quot;) Based on the mosaic plot, are views on the DREAM Act and political ideology independent? The vertical locations at which the ideological groups break into the Yes, No, and Not Sure categories differ, which indicates the variables are dependent. 6.2.3 Problem 3 Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable transplant indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called survived was used to indicate whether or not the patient was alive at the end of the study. The data is in the openintro package and is called heart_transplant. Create a mosaic plot. mosaic(survived~transplant,data=heart_transplant) Based on the mosaic plot, is survival independent of whether or not the patient got a transplant? Explain your reasoning. Proportion of patients who are alive at the end of the study is higher in the treatment group than in the control group. These data suggest that survival is not independent of whether or not the patient got a transplant. Using survtime create side-by-side boxplots for the control and treatment groups. heart_transplant %&gt;% gf_boxplot(survtime~transplant) %&gt;% gf_labs(title=&quot;Survival times for tranplant experiment&quot;, sub=&quot;Treatment group had the transplant&quot;,x=&quot;Tranplant&quot;,y=&quot;Survival time in days&quot;) %&gt;% gf_theme(theme_classic()) What do the box plots suggest about the efficacy (effectiveness) of transplants? The shape of the distribution of survival times in both groups is right skewed with one very clear outlier for the control group and other possible outliers in both groups on the high end. The median survival time for the control group is much lower than the median survival time for the treatment group; patients who got a transplant typically lived longer. Tying this together with the much lower variability in the control group, evident by a much smaller IQR than the treatment group (about 50 days versus 500 days), and we can see that patients who did not get a heart transplant tended to consistently die quite early relative to those who did have a transplant. Overall, very few patients without transplants made it beyond a year while nearly half of the transplant patients survived at least one year. It should also be noted that while the first and third quartiles of the treatment group is higher than those for the control group, the IQR for the treatment group is much bigger, indicating that there is more variability in survival times in the treatment group. "],["CS2.html", "Chapter 7 Case Study 7.1 Objectives 7.2 Homework", " Chapter 7 Case Study 7.1 Objectives Use R to simulate a probabilistic model. Use basic counting methods. 7.2 Homework 7.2.1 Problem 1 Exactly 2 people with the same birthday - Simulation. Complete a similar analysis for case where exactly 2 people in a room of 23 people have the same birthday. In this exercise you will use a computational simulation. Create a new R Markdown file and create a report. Yes, we know you could use this file but we want you to practice generating your own report. Simulate having 23 people in the class with each day of the year equally likely. Find the cases where exactly 2 people have the same birthday, you will have to alter the code from the Notes more than changing 18 to 23. Plot the frequency of occurrences as a bar chart. Estimate the probability of exactly two people having the same birthday. (do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% mutate(match=if_else(length==22,1,0)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.3601 (do(1000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% gf_bar(~length) 7.2.2 Problem 2 Exactly 2 people with the same birthday - Mathematical. Repeat problem 1 but do it mathematically. As a big hint, you will need to use the choose() function. The idea is that with 23 people we need to choose 2 of them to match. We thus need to multiply, the multiplication rule again, by choose(23,2). If you are having trouble, work with a total of 3 people in the room first. Find a formula to determine the exact probability of exactly 2 people in a room of 23 having the same birthday. Generalize your solution to any number n people in the room and create a function. Vectorize the function. Plot the probability of exactly 2 people having the same birthday versus number of people in the room. Comment on the shape of the curve and explain it. knit and compile your report. For two people we have choose(23,2)*prod(365:344)/365^23 ## [1] 0.3634222 exactly_two &lt;- function(n){ choose(n,2)*prod(365:(365-(n-2)))/365^n } exactly_two(23) ## [1] 0.3634222 exactly_two &lt;- Vectorize(exactly_two) gf_line(exactly_two(1:100)~ seq(1,100), xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of exactly least 2 people with matching birthdays&quot;) By the way, exactly three matches in simulation is hard. We have to table the data set.seed(10) temp &lt;- table(sample(days,size=23,replace = TRUE)) temp ## ## 13 24 50 72 92 110 137 143 154 155 211 231 263 271 285 330 338 342 344 351 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 ## 365 ## 1 (sum(temp==2) == 2)+0 ## [1] 1 (do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 3)==1)+0)) %&gt;% summarise(prob=mean(result)) ## prob ## 1 0.0117 Two sets that have same but different birthday (do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 2)==2)+0)) %&gt;% summarise(prob=mean(result)) ## prob ## 1 0.1139 (do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% mutate(match=if_else(length==21,1,0)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.1187 Mathematically exactly 3 is easy. Simulation seems to be off a little or the math formula is off. choose(23,3)*prod(365:345)/365^23 ## [1] 0.007395218 "],["PROBRULES.html", "Chapter 8 Probability Rules 8.1 Objectives 8.2 Homework", " Chapter 8 Probability Rules 8.1 Objectives Define and use properly in context all new terminology related to probability to include but not limited to: outcome, event, sample space, probability. Apply basic probability and counting rules to find probabilities. Describe the basic axioms of probability. Use R to calculate and simulate probabilities of events. 8.2 Homework 8.2.1 Problem 1 1. Let \\(A\\), \\(B\\) and \\(C\\) be events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), and \\(\\mbox{P}(C)=0.4\\). Also, we know that \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Find the following: \\(\\mbox{P}(A\\cup B)\\) \\[ \\mbox{P}(A\\cup B) = \\mbox{P}(A)+\\mbox{P}(B)-\\mbox{P}(A\\cap B)= 0.5+0.3-0.2 = 0.6 \\] \\(\\mbox{P}(A\\cup B \\cup C)\\) \\[ \\mbox{P}(A\\cup B \\cup C) = \\mbox{P}(A)+\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(A\\cap B)-\\mbox{P}(A\\cap C)-\\mbox{P}(B\\cap C)+\\mbox{P}(A\\cap B \\cap C) \\] \\[ = 0.5+0.3+0.4-0.2-0.12-0.1+0.05 = 0.83 \\] \\(\\mbox{P}(B&#39;\\cap C&#39;)\\) \\[ \\mbox{P}(B&#39;\\cap C&#39;)=\\mbox{P}((B\\cup C)&#39;) = 1-\\mbox{P}(B\\cup C) = 1-[\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(B\\cap C)] \\] \\[ = 1-(0.3+0.4-0.12) = 0.42 \\] \\(\\mbox{P}(A\\cup (B\\cap C))\\) \\[ \\mbox{P}(A\\cup (B\\cap C)) = \\mbox{P}(A)+\\mbox{P}(B\\cap C) -\\mbox{P}(A\\cap B \\cap C) = 0.5+0.12-0.05 = 0.57 \\] \\(\\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)\\) \\[ \\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)=\\mbox{P}(A\\cup B \\cup C)-\\mbox{P}(A\\cap B \\cap C) = 0.83-0.05 = 0.78 \\] 8.2.2 Problem 2 2. Consider the example of the family in the reading. What is the probability that the family has at least one boy? \\[ \\mbox{P}(\\mbox{at least one boy})=1-\\mbox{P}(\\mbox{no boys})=1-\\mbox{P}(\\mbox{GGG})=1-\\frac{1}{8} = 0.875 \\] 8.2.3 Problem 3 3. The Birthday Problem Revisited. Suppose there are \\(n=20\\) students in a classroom. My birthday, the instructor, is April 3rd. What is the probability that at least one student shares my birthday? Assume only 365 days in a year and assume that all birthdays are equally likely. \\[ \\mbox{P}(\\mbox{at least one other person shares my bday})=1-\\mbox{P}(\\mbox{no one else has my bday}) = \\] \\[ 1-\\left( \\frac{364}{365}\\right)^{20} = 0.0534 \\] In R, find the probability that at least one other person shares my birthday for each value of \\(n\\) from 1 to 80. Plot these probabilities with \\(n\\) on the \\(x\\)-axis and probability on the \\(y\\)-axis. At what value of \\(n\\) would the probability be at least 50%? Generalizing, \\[ \\mbox{P}(\\mbox{at least one other person shares my bday})=1-\\mbox{P}(\\mbox{no one else has my bday}) = 1-\\left( \\frac{364}{365}\\right)^{n} \\] n&lt;-1:300 mybday&lt;-function(x) 1-(364/365)^x mybday &lt;- Vectorize(mybday) Check our function. mybday(20) ## [1] 0.05339153 gf_line(mybday(n)~ n, xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of at least 1 person matching my birthday&quot;) %&gt;% gf_theme(theme_bw) prob &lt;- mybday(n) which(prob&gt;= .5) ## [1] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 ## [20] 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 ## [39] 291 292 293 294 295 296 297 298 299 300 So 253 people. 8.2.4 Problem 4 Thinking of the cards again. Answer the following questions: Define two events that are mutually exclusive. The first card drawn is red. The first card drawn is black. Define two events that are independent. The first card drawn is black. The first card drawn is a face card. Define an event and its complement. The first card drawn is less than 5. The first card drawn is equal to or more than 5. 8.2.5 Problem 5 Consider the license plate example from the reading. What is the probability that a license plate contains exactly one B? #fourth spot num4&lt;-10*10*10*1*25*25 #fifth spot num5&lt;-10*10*10*25*1*25 #sixth spot num6&lt;-10*10*10*25*25*1 denom&lt;-10*10*10*26*26*26 (num4+num5+num6)/denom ## [1] 0.1066796 What is the probability that a license plate contains at least one B? \\[ 1-\\mbox{P}(\\mbox{no B&#39;s}) \\] num0&lt;-10*10*10*25*25*25 1-num0/denom ## [1] 0.1110036 8.2.6 Problem 6 Consider the party example in the reading. Suppose 8 people showed up to the party dressed as zombies. What is the probability that all three awards are won by people dressed as zombies? \\[ \\frac{8\\cdot 7 \\cdot 6}{25\\cdot 24 \\cdot 23} \\] (8*7*6)/(25*24*23) ## [1] 0.02434783 What is the probability that zombies win most creative and funniest but not scariest? \\[ \\frac{8 \\cdot 17 \\cdot 7}{25 \\cdot 24 \\cdot 23} \\] (8*17*7)/(25*24*23) ## [1] 0.06898551 8.2.7 Problem 7 Consider the cards example from the reading. How many ways can we obtain a two pairs (2 of one number, 2 of another, and the final different)? We have to pick the rank of the two pairs. \\[\\binom{13}{2}\\] Notice here the order does matter because a pair of Kings and 4s is the same as a pair of 4s and Kings. This is different from the full house example. Make sure you understand this point. Now we have to pick two of the fours cards for each rank \\[\\binom{4}{2}\\binom{4}{2}\\] And finally we need the last card to come from the 44 remaining cards so that we dont get a full house. \\(\\binom{44}{1}\\) Putting it all together: \\(\\binom{13}{2}\\binom{4}{2}\\binom{4}{2}\\binom{44}{1}\\) choose(13,2)*choose(4,2)*choose(4,2)*choose(44,1) ## [1] 123552 What is the probability of drawing a four of a kind (four cards of the same value)? \\[ \\mbox{P}(\\mbox{4 of a kind})=\\frac{\\binom{13}{1}\\binom{4}{4}\\binom{48}{1}}{\\binom{52}{5}} \\] (13*1*48)/choose(52,5) ## [1] 0.000240096 8.2.8 Problem 8 Advanced Question: Consider rolling 5 dice. What is the probability of a pour resulting in a full house? First pick the value for the three of a kind, there are 6. Then pick the value from the remaining 5 for the two of a kind. This is actually a permutation. There are 30 distinct flavors of full house (three 1s &amp; two 2s, three 1s &amp; two 3s, etc.). In the reading we did this as \\[ \\binom{6}{1} \\times \\binom{5}{1} \\] We now have the 5 dice. We have to select three to have the same value and the order doesnt matter since they are the same value. Thus we multiple by \\(\\binom{5}{3}\\). Divide this by the total distinct ways the dice could have landed (assuming order matters). \\[ \\mbox{P}(\\mbox{full house}) = \\frac{30 \\times \\frac{5!}{3!2!}}{6^5} \\] \\[ \\mbox{P}(\\mbox{full house}) = \\frac{\\binom{6}{1} \\times \\binom{5}{1} \\times \\binom{5}{3}}{6^5} \\] 30*10/(6^5) ## [1] 0.03858025 Simulating is tough so lets write some code that may help. set.seed(23) temp&lt;-table(sample(1:6,size=5,replace=TRUE)) temp ## ## 1 3 4 5 ## 1 2 1 1 sum(temp==2) &amp; sum(temp==3) ## [1] FALSE temp&lt;-c(1,1,1,2,2) temp&lt;-table(temp) temp ## temp ## 1 2 ## 3 2 sum(temp==2) &amp; sum(temp==3) ## [1] TRUE Lets write a function. full_house &lt;-function(x){ temp&lt;-table(x) sum(temp==2) &amp; sum(temp==3) } temp&lt;-c(1,1,1,2,2) full_house(temp) ## [1] TRUE set.seed(751) results&lt;-do(10000)*full_house(sample(1:6,size=5,replace=TRUE)) mean(~full_house,data=results) ## [1] 0.039 "],["CONDPROB.html", "Chapter 9 Conditional Probability 9.1 Objectives 9.2 Homework", " Chapter 9 Conditional Probability 9.1 Objectives Define conditional probability and distinguish it from joint probability. Find a conditional probability using its definition. Using conditional probability, determine whether two events are independent. Apply Bayes Rule mathematically and via simulation. 9.2 Homework 9.2.1 Problem 1 1. Consider Exercise 1 from Lesson 2. Recall: \\(A\\), \\(B\\) and \\(C\\) are events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Are \\(A\\) and \\(B\\) independent? No. \\(\\mbox{P}(A)\\mbox{P}(B)=0.15\\neq \\mbox{P}(A\\cap B)\\). Are \\(B\\) and \\(C\\) independent? Yes. \\(\\mbox{P}(B)\\mbox{P}(C)=0.12 = \\mbox{P}(B\\cap C)\\). Also, \\[ \\mbox{P}(B|C)=\\frac{\\mbox{P}(B\\cap C)}{\\mbox{P}(C)}= 0.12/0.4 = 0.3 =\\mbox{P}(B) \\] 9.2.2 Problem 2 2. Suppose I have a biased coin (the probability I flip a heads is 0.6). I flip that coin twice. Assume that the coin is memoryless (flips are independent of one another). What is the probability that the second flip results in heads? 0.6 What is the probability that the second flip results in heads, given the first also resulted in heads? The coin is memoryless. So, \\[ \\mbox{P}(\\mbox{2nd flip heads}|\\mbox{1st flip heads}) = 0.6 \\] What is the probability both flips result in heads? Since the flips are independent, \\[ \\mbox{P}(\\mbox{both heads})=\\mbox{P}(\\mbox{1st flip heads})\\mbox{P}(\\mbox{2nd flip heads}) = 0.6*0.6=0.36 \\] What is the probability exactly one coin flip results in heads? This could happen in two ways. The first could be heads OR the second could be heads. \\[ \\mbox{P}(\\mbox{exactly one heads})=\\mbox{P}(\\mbox{1st flip heads})\\mbox{P}(\\mbox{2nd flip tails}) + \\mbox{P}(\\mbox{1st flip tails})\\mbox{P}(\\mbox{2nd flip heads}) \\] \\[ 0.6*0.4+0.4*0.6 = 0.48 \\] Now assume I flip the coin five times. What is the probability the result is 5 heads? \\[ \\mbox{P}(\\mbox{5 heads})= 0.6^5 = 0.0778 \\] 0.6^5 ## [1] 0.07776 What is the probability the result is exactly 2 heads (out of 5 flips)? There are \\(\\binom{5}{2} = 10\\) ways for this to happen ({HHTTT},{HTHTT},). So, \\[ \\mbox{P}(\\mbox{2 heads out of 5 flips})=\\binom{5}{2} 0.6^2(1-0.6)^3 = 0.2304 \\] choose(5,2)*0.6^2*0.4^3 ## [1] 0.2304 9.2.3 Problem 3 3. (Adapted from IPSUR, (Kerns 2010)). Suppose there are three assistants working at a company: Moe, Larry and Curly. All three assist with a filing process. Only one filing assistant is needed at a time. Moe assists 60% of the time, Larry assists 30% of the time and Curly assists the remaining 10% of the time. Occasionally, they make errors (misfiles); Moe has a misfile rate of 0.01, Larry has a misfile rate of 0.025, and Curly has a rate of 0.05. Suppose a misfile was discovered, but it is unknown who was on schedule when it occurred. Who is most likely to have committed the misfile? Calculate the probabilities for each of the three assistants. Let \\(E\\) be the event a misfile was committed. Also, let \\(M\\), \\(L\\), and \\(C\\) denote the events that Moe, Larry and Curly was the assistant at the time, respectively. \\[ \\mbox{P}(E)=\\mbox{P}(E \\cap M)+\\mbox{P}(E \\cap L)+\\mbox{P}(E\\cap C) \\] \\[ = \\mbox{P}(E|M)\\mbox{P}(M)+\\mbox{P}(E|L)\\mbox{P}(L)+\\mbox{P}(E|C)\\mbox{P}(C) = 0.01*0.6+0.025*0.3+0.05*0.1 = 0.0185 \\] Thus, \\[ \\mbox{P}(M|E)=\\frac{\\mbox{P}(E \\cap M)}{\\mbox{P}(E)}= \\frac{0.01*0.6}{0.0185}=0.3243 \\] Similarly, \\(\\mbox{P}(L|E)=0.4054\\) and \\(\\mbox{P}(C|E)=0.2702\\). Larry is the assistant most likely to have committed the error. 9.2.4 Problem 4 4. You are playing a game where there are two coins. One coin is fair and the other comes up heads 80% of the time. One coin is flipped 3 times and the result is three heads, what is the probability that the coin flipped is the fair coin? You will need to make an assumption about the probability of either coin being selected. Use Bayes formula to solve this problem. I will assume either coin is selected with a 50% probability. \\[ \\mbox{P}(Fair) = \\mbox{P}(Biased) = .5 \\] \\[ \\mbox{P}(3 Heads|Fair)=\\frac{1}{2}^3=\\frac{1}{8} \\] \\[ \\mbox{P}(3 Heads|Biased)=.8^3=0.512 \\] Now \\[ \\mbox{P}(Fair | 3 Heads) = \\frac{\\mbox{P}(3 Heads | Fair)\\mbox{P}(Fair)}{\\mbox{P}(3 Heads | Fair)\\mbox{P}(Fair)+\\mbox{P}(3 Heads| Biased)\\mbox{P}(Biased)} \\] Which is \\[ \\mbox{P}(Fair | 3 Heads) = \\frac{\\frac{1}{8}\\frac{1}{2}}{\\frac{1}{8}\\frac{1}{2}+.8^{3}\\frac{1}{2}} = 0.196 \\] .125*.5/(.125*.5+.8^3*.5) ## [1] 0.1962323 Use simulation to solve this problem. Lets use the same assumptions. We could do this problem in two ways. We could flip each coin a fixed number of times and combine the information or use a random process to pick a flipped coin and then flip it three times. Lets do the first. Lets flip a fair coin 50,000 times and count how many heads we get. set.seed(1154) data.frame(do(50000)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 6157 Now flip the biased coin. data.frame(do(50000)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 25743 So we have a total 6157 + 25743 heads of which 6157 came from the fair coin. Thus the probability of the coin being fair given 3 heads on the flips is: 6157/(6157 + 25743) ## [1] 0.1930094 Or 19.3%. Next pick a one of the coins with equal probability 100,000 times. set.seed(501) results &lt;- rflip(100000,summarize = TRUE) results ## n heads tails prob ## 1 1e+05 50226 49774 0.5 Now the fair coin was flipped 50226 times. Lets see how many times we get 3 heads when we flip that coin 3 times. data.frame(do(50226)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 6270 We have 6270 cases with 3 heads. Now for the biased coin. data.frame(do(49774)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 25512 Now we can determine the probability of a fair coin given 3 heads. 6270/(6270+25512) ## [1] 0.1972815 This code we could easily adapt if we dont think each coin is being selected with the same frequency. Suppose we think the fair coin has a 75% chance of being selected. The analysis would look like this: set.seed(9021) results &lt;- rflip(100000,prob=.75,summarize = TRUE) results ## n heads tails prob ## 1 1e+05 75023 24977 0.75 Now the fair coin was flipped 75023 times. Lets see how many times we get 3 heads when we flip that coin 3 times. data.frame(do(75023)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 9579 We have 9579 cases with 3 heads. Now for the biased coin. data.frame(do(24977)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 12789 Now we can determine the probability of a fair coin given 3 heads. 9579/(9579+12789) ## [1] 0.4282457 A much different answer. That is because prior to getting the data we believed the fair coin would be selected with a 75% probability. The data indicates that we need to update and lower this probability. We only flipped 3 times but the evidence is so in favor of the biased coin, that our probability dropped substantially. This is why Bayes is such a powerful tool. Think about what we just did with this problem. We started with a subjective believe that either coin would be selected with equal probability. This is called the prior probability. We then collected data on three flips of the coin. We used this empirical data to update our belief into a posterior probability. This is the basis for Bayesian statistical analysis. Bayesian statistics is an entire discipline unto itself. References "],["RANDVAR.html", "Chapter 10 Random Variables 10.1 Objectives 10.2 Homework", " Chapter 10 Random Variables 10.1 Objectives Define and use properly in context all new terminology. Given a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events. Simulate random variable for a discrete distribution. Find the moments of a discrete random variable. Find the expected value of a linear transformation of a random variable. 10.2 Homework 10.2.1 Problem 1 1. Suppose we are flipping a fair coin, and the result of a single coin flip is either heads or tails. Let \\(X\\) be a random variable representing the number of flips until the first heads. Is \\(X\\) discrete or continuous? What is the domain/support of \\(X\\)? \\(X\\) is discrete since number of flips is a discrete process (I cant perform a fraction of a flip). The wording is specific in that it is the number of flips until the first heads, so we must flip at least once. The domain of \\(X\\) is \\(S_X=\\{1,2,...\\}\\). What values do you expect \\(X\\) to take? What do you think is the average of \\(X\\)? Dont actually do any formal math, just think about if you were flipping a regular coin, how long it would take you to get the first heads. I would expect \\(X\\) to be 1 or 2 fairly often, since the coin is fair and has an even chance of landing on heads or tails. I would expect large values of \\(X\\) to be rare. For these reasons, I think the average of \\(X\\) should be around 2 flips or a little less than 2. Advanced: In R, generate 10,000 observations from \\(X\\). What is the average value of \\(X\\) based on this simulation? Note: There are many ways to do this. Below is a description of one approach. set.seed(68) which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[1] ## [1] 2 Now repeat using replicate() or do(). We will repeat 10000 times. results &lt;- do(10000)*which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[1] mean(~result,data=results) ## [1] 1.9849 tally(~result,data=results,format=&quot;percent&quot;) ## result ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 49.89 25.35 12.33 6.56 3.25 1.27 0.66 0.39 0.12 0.07 0.06 0.03 0.02 results %&gt;% gf_props(~result,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of flips&quot;, subtitle=&quot;Number of flips until first heads&quot;) As predicted, the mean is close to 2, and the most common values of \\(X\\) are 1 and 2. The most common is 1 occurring 50% of the time, this is what we would think since the coin comes up Heads 50% of the time. We know that \\(\\mbox{P}(X=1) = \\frac{1}{2}\\) and \\(\\mbox{P}(X=2) = \\frac{1}{2^2}\\) so in general \\(\\mbox{P}(X=x) = \\frac{1}{2^x}\\). This is the pmf. As an extra, to show that the sum of the infinite sequence of probabilities is 1 requires some Calculus knowledge. Lets start with a partial sum: \\[S_n=\\frac{1}{2}+\\frac{1}{4} +\\cdots + \\frac{1}{2^n}\\] Now multiply both sides by \\(\\frac{1}{2}\\). \\[\\frac{1}{2}S_n=\\frac{1}{4}+\\frac{1}{8} +\\cdots + \\frac{1}{2^{n+1}}\\] The difference between these two sums is \\[S_n-\\frac{1}{2}S_n=\\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}}\\] Now as \\[\\lim_{n \\to +\\infty} \\frac{1}{2^{n+1}} = 0\\] So \\[\\lim_{n \\to +\\infty} \\left[ \\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}} \\right]\\] This implies that \\(S = 1\\). 10.2.2 Problem 2 2. Repeat Problem 1, except part d, but with a different random variable, \\(Y\\): the number of coin flips until the fifth heads. \\(Y\\) is discrete for the same reasons as \\(X\\). The domain of \\(Y\\) is \\(S_Y=\\{5,6,...\\}\\). In order to land on heads five times, it would be reasonable to expect around 9 to 13 flips. Thus, I would expect \\(Y\\) to take values 8, 9, 10, 11, and 12 fairly often, and values outside of that range less often. I think the average of \\(Y\\) should be around 10 or so. set.seed(102) results &lt;- do(10000)*which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[5] mean(~result,data=results) ## [1] 9.9728 tally(~result,data=results,format=&quot;percent&quot;) ## result ## 5 6 7 8 9 10 11 12 13 14 15 16 17 ## 3.06 8.21 12.14 13.26 13.71 11.52 10.74 8.17 6.06 4.50 3.02 1.86 1.32 ## 18 19 20 21 22 23 24 25 28 29 ## 0.88 0.65 0.31 0.21 0.16 0.12 0.04 0.04 0.01 0.01 results %&gt;% gf_props(~result,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of flips&quot;, subtitle=&quot;Number of flips until 5th heads&quot;) The most common values of \\(Y\\) are between 6 and 11. The average of \\(Y\\) in this simulation is 9.97, close to what we predicted. The pmf is not that bad but you must know about the binomial distribution first. If we get the fifth heads on the nth flip, the prior n-1 flips are a binomial with n-1 successes. The final flip is a success so we multiply the binomial by the probability of success. 10.2.3 Problem 3 3. Suppose you are a data analyst for a large international airport. Your boss, the head of the airport, is dismayed that this airport has received negative attention in the press for inefficiencies and sluggishness. In a staff meeting, your boss gives you a week to build a report addressing the timeliness at the airport. Your boss is in a big hurry and gives you no further information or guidance on this task. Prior to building the report, you will need to conduct some analysis. To aid you in this, create a list of at least three random variables that will help you address timeliness at the airport. For each of your random variables, Determine whether it is discrete or continuous. Report its domain. What is the experimental unit? Explain how this random variable will be useful in addressing timeliness at the airport. I will provide one example: Let \\(D\\) be the difference between a flights actual departure and its scheduled departure. This is a continuous random variable, since time can be measured in fractions of minutes. A flight can be early or late, so domain is any real number. The experimental unit is each individual (non-canceled) flight. This is a useful random variable because the average value of \\(D\\) will describe whether flights take off on time. We could also find out how often \\(D\\) exceeds 0 (implying late departure) or how often \\(D\\) exceeds 30 minutes, which could indicate a very late departure. There are many correct answers. \\(X\\): Time it takes for a passenger to go through security (defined as time from entering security line to departing security with all belongings). Continuous. Experimental unit is individual passenger. This variable would help identify whether security line is too long. We could also explore how \\(X\\) changes based on day or time of day. \\(Y\\): Status of each scheduled departure (on time, somewhat late, very late, canceled). Discrete. Experimental unit is each scheduled departure. This variable will help describe how often flights are canceled or late. We could also explore \\(Y\\) by airline, destination, time of day, etc. \\(Z\\): Number of time-related complaints at customer service desk in a given day. Discrete. Experimental unit is day. This variable will describe attitudes/perceptions of customers. It is probably a bad sign if customers feel like the airport is not working efficiently. We can explore how \\(Z\\) changes over time. 10.2.4 Problem 4 4. Consider the experiment of rolling two fair six-sided dice. Let the random variable \\(Y\\) be the absolute difference between the two numbers that appear upon rolling the dice. What is the domain/support of \\(Y\\)? \\(S_Y=\\{0,1,2,3,4,5\\}\\). What values do you expect \\(Y\\) to take? What do you think is the average of \\(Y\\)? Dont actually do any formal math, just think about the experiment. Id say that \\(Y\\) should take values 0,1 and 2 fairly often. Id guess that the average should be around 1.5. Find the probability mass function and cumulative distribution function of \\(Y\\). Using counting methods, we know there are 36 possible values. We can just count them. The number 0 will occur when both numbers are the same, which happens six times. The number 1 happens when the first die is one larger than the second, 5 times, or vice versa. Thus 1 happens 10 times. Continue this process. Thus, the pmf of \\(Y\\) becomes: \\[ f_Y(y)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{6}{36}, &amp; y=0 \\\\ \\frac{10}{36}, &amp; y=1 \\\\ \\frac{8}{36}, &amp; y=2 \\\\ \\frac{6}{36}, &amp; y=3 \\\\ \\frac{4}{36}, &amp; y=4 \\\\ \\frac{2}{36}, &amp; y=5 \\\\ 0, &amp; \\mbox{otherwise} \\end{array} \\right . \\] We could also create a table and count the entries. \\[ \\begin{array}{cc|cccccc} &amp; &amp; &amp; &amp;\\textbf{Die} &amp; \\textbf{2} \\\\ &amp; &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\&amp;\\hline 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\\\textbf{Die 1} &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 \\\\&amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\\\&amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 \\\\&amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 \\\\&amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 \\end{array} \\] The cdf of \\(Y\\) is thus, \\[ F_Y(y)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, &amp; y &lt; 0 \\\\ \\frac{6}{36}, &amp; 0\\leq y &lt;1 \\\\ \\frac{16}{36}, &amp; 1\\leq y &lt;2 \\\\ \\frac{24}{36}, &amp; 2 \\leq y &lt;3 \\\\ \\frac{30}{36}, &amp; 3 \\leq y &lt;4 \\\\ \\frac{34}{36}, &amp; 4 \\leq y &lt;5 \\\\ \\frac{36}{36}, &amp; y\\geq 5 \\end{array} \\right . \\] Find the expected value and variance of \\(Y\\). \\[ \\mbox{E}(Y)=\\sum_{y=0}^5 y\\mbox{P}(Y=y) = 0\\times {6\\over 36} + 1 \\times {10\\over 36} + 2\\times {8\\over 36} + 3\\times {6\\over 36} + 4 \\times {4\\over 36} + 5 \\times {2\\over 36} = \\] \\[ {70\\over 36} = 1.944 \\] y&lt;-c(0,1,2,3,4,5) mean_y&lt;-sum(y*c(6,10,8,6,4,2)/36) mean_y ## [1] 1.944444 The variance is: sum((y-mean_y)^2*(c(6,10,8,6,4,2)/36)) ## [1] 2.052469 Advanced: In R, obtain 10,000 realizations of \\(Y\\). In other words, simulate the roll of two fair dice, record the absolute difference and repeat this 10,000 times. Construct a frequency table of your results (what percentage of time did you get a difference of 0? difference of 1? etc.) Find the mean and variance of your simulated sample of \\(Y\\). Were they close to your answers in part d? set.seed(9) sim_diffs&lt;-do(10000)*abs(diff(sample(1:6,2,replace=T))) tally(~abs,data=sim_diffs,format=&quot;proportion&quot;) ## abs ## 0 1 2 3 4 5 ## 0.1643 0.2752 0.2273 0.1618 0.1116 0.0598 mean(~abs,data=sim_diffs) ## [1] 1.9606 var(sim_diffs)*9999/10000 ## abs ## abs 2.077248 true_mean&lt;-sum(c(6,10,8,6,4,2)/36*c(0,1,2,3,4,5)) true_mean ## [1] 1.944444 sum(c(6,10,8,6,4,2)/36*(c(0,1,2,3,4,5)-true_mean)^2) ## [1] 2.052469 We got similar mean and variance to the theoretical values. 10.2.5 Problem 5 5. Prove the Lemma from the Notes: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Show \\(\\mbox{E}(aX + b)=a\\mbox{E}(X)+b\\). \\[ \\mbox{E}(aX+b)=\\sum_x (ax+b)f_X(x) = \\sum_x axf_X(x)+\\sum_x bf_X(x) + a\\sum_x xf_X(x)+b\\sum_x f_X(x) \\] Since \\(\\sum_x xf_X(x) = \\mbox{E}(X)\\) and \\(\\sum_x f_X(x)=1\\), this reduces to \\(a\\mbox{E}(X)+b\\). \\[ \\mbox{Var}(aX+b)=\\mbox{E}\\left[(aX+b-\\mbox{E}(aX+b))^2\\right]=\\mbox{E}\\left[(aX+b-a\\mbox{E}(X)-b)^2\\right]=\\mbox{E}\\left[(aX-a\\mbox{E}(X)^2\\right] \\] \\[ =\\mbox{E}\\left[a^2(X-\\mbox{E}(X))^2\\right]=a^2\\mbox{E}\\left[(X-\\mbox{E}(X))^2\\right]=a^2\\mbox{Var}(X) \\] 10.2.6 Problem 6 6. In the Notes, we saw that \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show that \\(\\mbox{Var}(X)\\) is also equal to \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\). \\[ \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]=\\mbox{E}[X^2-2\\mu_XX+\\mu_X^2] = \\mbox{E}(X^2)-\\mbox{E}(2\\mu_XX)+\\mbox{E}(\\mu_X^2) \\] The quantity \\(\\mu_X\\) is a constant with respect to \\(X\\), so \\[ =\\mbox{E}(X^2)-2\\mu_X\\mbox{E}(X)+\\mu_X^2=\\mbox{E}(X^2)-2\\mu_X^2+\\mu_X^2 = \\mbox{E}(X^2)-\\mu_X^2 \\] "],["CONRANDVAR.html", "Chapter 11 Continuous Random Variables 11.1 Objectives 11.2 Homework", " Chapter 11 Continuous Random Variables 11.1 Objectives Define and properly use the new terms to include probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables. Given a continuous random variable, find probabilities using the pdf and/or the cdf. Find the mean and variance of a continuous random variable. 11.2 Homework 11.2.1 Problem 1 1. Let \\(X\\) be a continuous random variable on the domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\). Assume that \\(f(x)\\) is a valid pdf. Find the value of \\(k\\). Because \\(f\\) is a valid pdf, we know that \\(\\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = 1\\). So, \\[ \\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^3}{54}\\bigg|_{-k}^k = \\frac{k^3}{54}-\\frac{-k^3}{54}=\\frac{k^3}{27}=1 \\] Thus, \\(k=3\\). Using R, see if you can follow the code. my_pdf &lt;- function(x)integrate(function(y)y^2/18,-x,x)$value my_pdf&lt;-Vectorize(my_pdf) domain &lt;- seq(.01,5,.1) gf_line(my_pdf(domain)~domain) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Cumulative probability for different values of k&quot;,x=&quot;k&quot;,y=&quot;Cummulative Probability&quot;) %&gt;% gf_hline(yintercept = 1,color = &quot;blue&quot;) Looks like \\(k \\approx 3\\) from the plot. uniroot(function(x)my_pdf(x)-1,c(-10,10))$root ## [1] 2.999997 Plot the pdf of \\(X\\). x&lt;-seq(-3,3,0.001) fx&lt;-x^2/18 gf_line(fx~x,ylab=&quot;f(x)&quot;,title=&quot;pdf of X&quot;) %&gt;% gf_theme(theme_classic()) ggplot(data.frame(x=c(-3, 3)), aes(x)) + stat_function(fun=function(x) x^2/18) + theme_classic() + labs(y=&quot;f(x)&quot;,title=&quot;pdf of X&quot;) curve(x^2/18,from=-3,to=3,ylab=&quot;f(x)&quot;,main=&quot;pdf of X&quot;) Find and plot the cdf of \\(X\\). \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\int_{-3}^x \\frac{t^2}{18}\\mathop{}\\!\\mathrm{d}t = \\frac{t^3}{54}\\bigg|_{-3}^x = \\frac{x^3}{54}+\\frac{1}{2} \\] \\[ F_X(x)=\\left\\{\\begin{array}{ll} 0, &amp; x&lt;-3 \\\\ \\frac{x^3}{54}+\\frac{1}{2}, &amp; -3\\leq x \\leq 3 \\\\ 1, &amp; x&gt;3 \\end{array}\\right. \\] x&lt;-seq(-3.5,3.5,0.001) fx&lt;-pmin(1,(1*(x&gt;=-3)*(x^3/54+1/2))) gf_line(fx~x,ylab=&quot;F(x)&quot;,title=&quot;cdf of X&quot;) %&gt;% gf_theme(theme_classic()) Find \\(\\mbox{P}(X&lt;1)\\). \\[ \\mbox{P}(X&lt;1)=F(1)=\\frac{1}{54}+\\frac{1}{2}=0.519 \\] integrate(function(x)x^2/18,-3,1) ## 0.5185185 with absolute error &lt; 5.8e-15 Find \\(\\mbox{P}(1.5&lt;X\\leq 2.5)\\). \\[ \\mbox{P}(1.5&lt; X \\leq 2.5)=F(2.5)-F(1.5)=\\frac{2.5^3}{54}+\\frac{1}{2}-\\frac{1.5^3}{54}-\\frac{1}{2}=0.227 \\] integrate(function(x)x^2/18,1.5,2.5) ## 0.2268519 with absolute error &lt; 2.5e-15 Find the 80th percentile of \\(X\\) (the value \\(x\\) for which 80% of the distribution is to the left of that value). Need \\(x\\) such that \\(F(x)=0.8\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.8\\) for \\(x\\) yields \\(x=2.530\\). uniroot(function(x)x^3/54+.5-.8,c(-3,3)) ## $root ## [1] 2.530293 ## ## $f.root ## [1] -1.854422e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Find the value \\(x\\) such that \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\). Because this distribution is symmetric, finding \\(x\\) is equivalent to finding \\(x\\) such that \\(\\mbox{P}(X&gt;x)=0.3\\). (It helps to draw a picture). Thus, we need \\(x\\) such that \\(F(x)=0.7\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.7\\) for \\(x\\) yields \\(x=2.210\\). Find the mean and variance of \\(X\\). \\[ \\mbox{E}(X)=\\int_{-3}^3 x\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{72}\\bigg|_{-3}^3=\\frac{81}{72}-\\frac{81}{72} = 0 \\] \\[ \\mbox{E}(X^2)=\\int_{-3}^3 x^2\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^5}{90}\\bigg|_{-3}^3=\\frac{243}{90}-\\frac{-243}{90} = 5.4 \\] \\[ \\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2=5.4-0^2=5.4 \\] Simulate 10000 values from this distribution and plot the density. This is tricky since we need a cube root function. Just raising to the one-third power wont work. Lets write our own function. cuberoot &lt;- function(x) { sign(x) * abs(x)^(1/3)} set.seed(4) results &lt;- do(10000)*cuberoot((runif(1)-.5)*54) results %&gt;% gf_dens(~cuberoot) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;pdf from simulation&quot;,x=&quot;x&quot;,y=&quot;f(x)&quot;) Notice that the smoothing operation goes past the support of \\(X\\) and thus shows a concave down curve. We could clean up by limiting the x-axis to the interval [-3,3]. inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max ## ...1 cuberoot numeric -2.999981 -2.382864 -0.1574198 2.376346 2.999347 ## mean sd n missing ## ...1 -0.002416475 2.322639 10000 0 11.2.2 Problem 2 2. Let \\(X\\) be a continuous random variable. Prove that the cdf of \\(X\\), \\(F_X(x)\\) is a non-decreasing function. (Hint: show that for any \\(a &lt; b\\), \\(F_X(a) \\leq F_X(b)\\).) Let \\(a&lt;b\\), where \\(a\\) and \\(b\\) are both in the domain of \\(X\\). Note that \\(F_X(a)=\\mbox{P}(X\\leq a)\\) and \\(F_X(b)=\\mbox{P}(X\\leq b)\\). Since \\(a&lt;b\\), we can partition \\(\\mbox{P}(X\\leq b)\\) as \\(\\mbox{P}(X\\leq a)+\\mbox{P}(a &lt; X \\leq b)\\). One of the axioms of probability is that a probability must be non-negative, so I know that \\(\\mbox{P}(a &lt; X \\leq b)\\geq 0\\). Thus, \\[ \\mbox{P}(X\\leq b)=\\mbox{P}(X\\leq a)+\\mbox{P}(a &lt; X \\leq b) \\geq \\mbox{P}(X\\leq a) \\] So, we have shown that \\(F_X(a)\\leq F_X(b)\\). Thus, \\(F_X(x)\\) is a non-decreasing function. "],["references.html", "References", " References "]]
